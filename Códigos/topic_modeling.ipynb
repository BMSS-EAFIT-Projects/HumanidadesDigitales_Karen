{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "649f0efa",
   "metadata": {},
   "source": [
    "Fecha: Julio de 2025\n",
    "\n",
    "Se busca hacer análisis de frecuencias en las columnas de opinión por año, mes y periódico. E implementar modelado de tópicos. Primer método LDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "106015a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyLDAvis in c:\\users\\karen\\anaconda3\\lib\\site-packages (3.4.1)\n",
      "Requirement already satisfied: numpy>=1.24.2 in c:\\users\\karen\\anaconda3\\lib\\site-packages (from pyLDAvis) (1.26.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\karen\\anaconda3\\lib\\site-packages (from pyLDAvis) (1.11.4)\n",
      "Requirement already satisfied: pandas>=2.0.0 in c:\\users\\karen\\anaconda3\\lib\\site-packages (from pyLDAvis) (2.2.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\karen\\anaconda3\\lib\\site-packages (from pyLDAvis) (1.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\karen\\anaconda3\\lib\\site-packages (from pyLDAvis) (3.1.4)\n",
      "Requirement already satisfied: numexpr in c:\\users\\karen\\anaconda3\\lib\\site-packages (from pyLDAvis) (2.8.7)\n",
      "Requirement already satisfied: funcy in c:\\users\\karen\\anaconda3\\lib\\site-packages (from pyLDAvis) (2.0)\n",
      "Requirement already satisfied: scikit-learn>=1.0.0 in c:\\users\\karen\\anaconda3\\lib\\site-packages (from pyLDAvis) (1.3.0)\n",
      "Requirement already satisfied: gensim in c:\\users\\karen\\anaconda3\\lib\\site-packages (from pyLDAvis) (4.3.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\karen\\anaconda3\\lib\\site-packages (from pyLDAvis) (75.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\karen\\anaconda3\\lib\\site-packages (from pandas>=2.0.0->pyLDAvis) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\karen\\anaconda3\\lib\\site-packages (from pandas>=2.0.0->pyLDAvis) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\karen\\anaconda3\\lib\\site-packages (from pandas>=2.0.0->pyLDAvis) (2023.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\karen\\anaconda3\\lib\\site-packages (from scikit-learn>=1.0.0->pyLDAvis) (3.5.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\karen\\anaconda3\\lib\\site-packages (from gensim->pyLDAvis) (5.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\karen\\anaconda3\\lib\\site-packages (from jinja2->pyLDAvis) (2.1.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\karen\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=2.0.0->pyLDAvis) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda8799e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyLDAvis'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[110], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlang\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstop_words\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m STOP_WORDS\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyLDAvis\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msklearn\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pyLDAvis'"
     ]
    }
   ],
   "source": [
    "# Librerías\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import re\n",
    "from spacy.lang.es.stop_words import STOP_WORDS\n",
    "import spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "21399b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ruta del directorio que contiene los archivos CSV\n",
    "folder_path = r'C:\\Users\\karen\\Documents\\HumanidadesDigitales_git\\BDD_Corpus\\BDD_CSV'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e4567fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = []\n",
    "\n",
    "for archivo in os.listdir(folder_path):\n",
    "    if archivo.endswith('.csv'):\n",
    "        file_path = os.path.join(folder_path, archivo)        \n",
    "        try:\n",
    "            df = pd.read_csv(file_path, encoding='latin-1', sep=';')  \n",
    "            dataframes.append(df)\n",
    "        except Exception as e:\n",
    "            print(f'Error al procesar {file_path}: {e}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "78138bb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Diario</th>\n",
       "      <th>Autor</th>\n",
       "      <th>Fecha</th>\n",
       "      <th>Título</th>\n",
       "      <th>Texto</th>\n",
       "      <th>Vínculo</th>\n",
       "      <th>Diario</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>El Espectador</td>\n",
       "      <td>Alberto Donadio</td>\n",
       "      <td>30 de diciembre de 2017</td>\n",
       "      <td>César Gaviria y César Mondragón</td>\n",
       "      <td>César Gaviria y César Mondragón son expresiden...</td>\n",
       "      <td>https://web.archive.org/web/20180101101206/htt...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>El Espectador</td>\n",
       "      <td>Antonio Casale</td>\n",
       "      <td>30 de diciembre de 2017</td>\n",
       "      <td>Más recomendaciones de fin de año</td>\n",
       "      <td>Un libro y dos documentales imperdibles para e...</td>\n",
       "      <td>https://web.archive.org/web/20180101101206/htt...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>El Espectador</td>\n",
       "      <td>Gonzalo Hernández</td>\n",
       "      <td>1 de enero de 2018</td>\n",
       "      <td>Fajardo: para nada tibio</td>\n",
       "      <td>La Coalición Colombia Partido Alianza Verde, ...</td>\n",
       "      <td>https://web.archive.org/web/20180102104221/htt...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>El Espectador</td>\n",
       "      <td>Eduardo Barajas Sandoval</td>\n",
       "      <td>1 de enero de 2018</td>\n",
       "      <td>Macedonia de Norte</td>\n",
       "      <td>Las interpretaciones de la historia sirven com...</td>\n",
       "      <td>https://web.archive.org/web/20180102104221/htt...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>El Espectador</td>\n",
       "      <td>Daniel Emilio Rojas Castro</td>\n",
       "      <td>1 de enero de 2018</td>\n",
       "      <td>El nacionalismo según Vargas Llosa</td>\n",
       "      <td>La semana pasada Mario Vargas Llosa publicó un...</td>\n",
       "      <td>https://web.archive.org/web/20180102104221/htt...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Diario                       Autor                    Fecha  \\\n",
       "0  El Espectador             Alberto Donadio  30 de diciembre de 2017   \n",
       "1  El Espectador              Antonio Casale  30 de diciembre de 2017   \n",
       "2  El Espectador           Gonzalo Hernández       1 de enero de 2018   \n",
       "3  El Espectador    Eduardo Barajas Sandoval       1 de enero de 2018   \n",
       "4  El Espectador  Daniel Emilio Rojas Castro       1 de enero de 2018   \n",
       "\n",
       "                               Título  \\\n",
       "0     César Gaviria y César Mondragón   \n",
       "1   Más recomendaciones de fin de año   \n",
       "2            Fajardo: para nada tibio   \n",
       "3                  Macedonia de Norte   \n",
       "4  El nacionalismo según Vargas Llosa   \n",
       "\n",
       "                                               Texto  \\\n",
       "0  César Gaviria y César Mondragón son expresiden...   \n",
       "1  Un libro y dos documentales imperdibles para e...   \n",
       "2  La Coalición Colombia Partido Alianza Verde, ...   \n",
       "3  Las interpretaciones de la historia sirven com...   \n",
       "4  La semana pasada Mario Vargas Llosa publicó un...   \n",
       "\n",
       "                                             Vínculo Diario   \n",
       "0  https://web.archive.org/web/20180101101206/htt...     NaN  \n",
       "1  https://web.archive.org/web/20180101101206/htt...     NaN  \n",
       "2  https://web.archive.org/web/20180102104221/htt...     NaN  \n",
       "3  https://web.archive.org/web/20180102104221/htt...     NaN  \n",
       "4  https://web.archive.org/web/20180102104221/htt...     NaN  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenar todos los DataFrames en uno solo\n",
    "if dataframes:\n",
    "    corpus_completo = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "corpus_completo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956b1fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocesamiento de los datos\n",
    "\n",
    "# Cargar modelo de lenguaje en español de Spacy, el modelo más pequeño\n",
    "nlp = spacy.load(\"es_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e6826b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocesar_texto(texto):\n",
    "    # Limpieza para eliminar URLs, emails, caracteres especiales\n",
    "    texto = re.sub(r'http\\S+|www\\S+|https\\S+', '', texto, flags=re.MULTILINE)\n",
    "    texto = re.sub(r'\\S+@\\S+', '', texto)\n",
    "    texto = re.sub(r'[^\\w\\sáéíóúñÁÉÍÓÚÑ]', ' ', texto)\n",
    "    \n",
    "    # Procesamiento con spaCy\n",
    "    doc = nlp(texto.lower())\n",
    "    \n",
    "    # Lematización y filtrado\n",
    "    tokens = [\n",
    "        token.lemma_ for token in doc\n",
    "        if not token.is_stop \n",
    "        and not token.is_punct \n",
    "        and not token.is_space\n",
    "        and len(token.lemma_) > 2  # Eliminar palabras muy cortas\n",
    "        and token.lemma_.isalpha()  # Solo palabras alfabéticas\n",
    "    ]\n",
    "    \n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "9af1a571",
   "metadata": {},
   "outputs": [],
   "source": [
    "titulos_procesados = corpus_completo['Título'].astype(str).apply(preprocesar_texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "c13d8403",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        césar gavirio césar mondragón\n",
       "1                    recomendación año\n",
       "2                        fajardo tibio\n",
       "3                      macedonia norte\n",
       "4             nacionalismo vargas llós\n",
       "                     ...              \n",
       "13339          capitalismo escandinava\n",
       "13340          derrotar guacho amazona\n",
       "13341               porte arma derecho\n",
       "13342                         congreso\n",
       "13343           casanarar bicentenario\n",
       "Name: Título, Length: 13344, dtype: object"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titulos_procesados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57dd825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorización (creación de matriz documento-término)\n",
    "vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=2000)\n",
    "X = vectorizer.fit_transform(titulos_procesados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "7f4b2fff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tópico #0:\n",
      "paz guerra política gazapera pandemia mujer vida elección pasar niño\n",
      "\n",
      "Tópico #1:\n",
      "duque presidente país trump bogotá justicia iván futuro prima ganar\n",
      "\n",
      "Tópico #2:\n",
      "colombia democracia tiempo tola maruja derecho educación reforma miedo crisis\n",
      "\n",
      "Tópico #3:\n",
      "año político petro gobierno voto mundo economía volver corrupción hora\n",
      "\n",
      "Tópico #4:\n",
      "historia nacional fiscal uribir venezuela colombiano ley muerte cambio covid\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Modelado de tópicos con LDA\n",
    "lda = LatentDirichletAllocation(n_components=5, random_state=42)\n",
    "lda.fit(X)\n",
    "\n",
    "# Función para mostrar los tópicos\n",
    "def mostrar_topicos(modelo, vectorizer, n_palabras=10):\n",
    "    palabras = vectorizer.get_feature_names_out()\n",
    "    for idx, topico in enumerate(modelo.components_):\n",
    "        print(f\"Tópico #{idx}:\")\n",
    "        print(\" \".join([palabras[i] for i in topico.argsort()[:-n_palabras - 1:-1]]))\n",
    "        print()\n",
    "\n",
    "mostrar_topicos(lda, vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "13965d19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tópico #0:\n",
      "paz petro tiempo tola maruja tola maruja historia venezuela miedo educación\n",
      "\n",
      "Tópico #1:\n",
      "político vida bogotá justicia pasar palabra ley matar lección agua\n",
      "\n",
      "Tópico #2:\n",
      "duque presidente guerra política gobierno voto covid mujer iván público\n",
      "\n",
      "Tópico #3:\n",
      "democracia gazapera economía corrupción muerte memoria jep problema uribir centro\n",
      "\n",
      "Tópico #4:\n",
      "colombia país año social trump pandemia niño ganar reforma nacional\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Vectorización incluyendo bigrams\n",
    "vectorizer_bigrams = CountVectorizer(ngram_range=(1, 2), max_df=0.85, min_df=2)\n",
    "X_bigrams = vectorizer_bigrams.fit_transform(titulos_procesados)\n",
    "\n",
    "# LDA con bigrams\n",
    "lda_bigrams = LatentDirichletAllocation(n_components=5, random_state=42)\n",
    "lda_bigrams.fit(X_bigrams)\n",
    "\n",
    "mostrar_topicos(lda_bigrams, vectorizer_bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "f3cc2f8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Casanare y el bicentenario'"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_completo.iloc[13343]['Título']  # Ejemplo de título original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "13687f8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherencia: 0.441\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import CoherenceModel\n",
    "import numpy as np\n",
    "\n",
    "# 1. Obtener los términos más importantes por tópico\n",
    "def get_top_words_lda(model, feature_names, n_top_words):\n",
    "    top_words = []\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_features_ind = topic.argsort()[:-n_top_words - 1:-1]\n",
    "        top_words.append([feature_names[i] for i in top_features_ind])\n",
    "    return top_words\n",
    "\n",
    "# 2. Obtener los nombres de las características (palabras) del vectorizer\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# 3. Obtener las palabras más relevantes por tópico (ej: 10 palabras por tópico)\n",
    "top_words_per_topic = get_top_words_lda(lda, feature_names, n_top_words=10)\n",
    "\n",
    "# 4. Asegurarse de que los textos están tokenizados correctamente\n",
    "textos_tokenizados = [texto.split() for texto in titulos_procesados]\n",
    "\n",
    "# 5. Crear el diccionario de Gensim\n",
    "dictionary = Dictionary(textos_tokenizados)\n",
    "\n",
    "# 6. Calcular la coherencia\n",
    "coherence_model = CoherenceModel(\n",
    "    topics=top_words_per_topic,  # Lista de listas de palabras\n",
    "    texts=textos_tokenizados,    # Lista de listas de tokens\n",
    "    dictionary=dictionary,       # Objeto Dictionary de Gensim\n",
    "    coherence='c_v'\n",
    ")\n",
    "\n",
    "print(f\"Coherencia: {coherence_model.get_coherence():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "4c352c2a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyLDAvis.sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[128], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyLDAvis\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msklearn\u001b[39;00m\n\u001b[0;32m      2\u001b[0m pyLDAvis\u001b[38;5;241m.\u001b[39menable_notebook()\n\u001b[0;32m      3\u001b[0m vis \u001b[38;5;241m=\u001b[39m pyLDAvis\u001b[38;5;241m.\u001b[39msklearn\u001b[38;5;241m.\u001b[39mprepare(lda, X, vectorizer)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pyLDAvis.sklearn'"
     ]
    }
   ],
   "source": [
    "import pyLDAvis.sklearn\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.sklearn.prepare(lda, X, vectorizer)\n",
    "vis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
