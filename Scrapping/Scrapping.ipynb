{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "fraZdbMlRsMY"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from openpyxl import Workbook\n",
        "from openpyxl.utils.dataframe import dataframe_to_rows\n",
        "import time\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "uDWp2Vi9dZoA"
      },
      "outputs": [],
      "source": [
        "headers = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vZnwtGJVSPzS",
        "outputId": "14710ee5-9c2a-45c9-fb17-dfeaf9467bcd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Título: La descentralización en jaque (tras escena del crimen de Karina García)\n",
            "Fecha: Fecha no encontrada\n",
            "Autor: Autor no encontrado\n",
            "\n",
            "--- CONTENIDO ---\n",
            "\n",
            "Contenido no encontrado\n"
          ]
        }
      ],
      "source": [
        "# URL del artículo en Wayback Machine\n",
        "url = \"https://web.archive.org/web/20190910025751/https://www.elespectador.com/opinion/la-descentralizacion-en-jaque-tras-escena-del-crimen-de-karina-garcia-columna-880135\"\n",
        "\n",
        "response = requests.get(url)\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "# Diccionario para meses en español\n",
        "meses = {\n",
        "    \"Ene\": \"enero\", \"Feb\": \"febrero\", \"Mar\": \"marzo\", \"Abr\": \"abril\",\n",
        "    \"May\": \"mayo\", \"Jun\": \"junio\", \"Jul\": \"julio\", \"Ago\": \"agosto\",\n",
        "    \"Sep\": \"septiembre\", \"Oct\": \"octubre\", \"Nov\": \"noviembre\", \"Dic\": \"diciembre\"\n",
        "}\n",
        "\n",
        "# 1. Extraer título\n",
        "titulo = soup.find('h1').get_text(strip=True)\n",
        "\n",
        "# 2. Extraer fecha formateada\n",
        "fecha_element = soup.find('div', class_='node-post-date')\n",
        "if fecha_element:\n",
        "    fecha_texto = fecha_element.get_text(strip=True).split(' - ')[0]\n",
        "    dia, mes_abrev, anio = fecha_texto.split()\n",
        "    fecha_formateada = f\"{dia} de {meses[mes_abrev]} de {anio}\"\n",
        "else:\n",
        "    fecha_formateada = \"Fecha no encontrada\"\n",
        "\n",
        "# 3. Extraer autor (texto después de \"Por:\")\n",
        "autor_element = soup.find('span', class_='by')  # Localiza el span con \"Por:\"\n",
        "if autor_element:\n",
        "    autor = autor_element.next_sibling.strip()  # Toma el texto HERMANO siguiente al span\n",
        "else:\n",
        "    autor = \"Autor no encontrado\"\n",
        "\n",
        "# 4. Extraer contenido limpio con párrafos (versión mejorada)\n",
        "contenido_div = soup.find('div', class_='node-body')\n",
        "if contenido_div:\n",
        "    # Primero eliminar el div no deseado si existe\n",
        "    info_node = contenido_div.find('div', class_='info_node_hide')\n",
        "    if info_node:\n",
        "        info_node.decompose()  # Esto elimina completamente el div y su contenido\n",
        "\n",
        "    # Eliminar solo elementos no deseados (scripts, iframes, etc.)\n",
        "    for element in contenido_div(['script', 'style', 'iframe', 'img', 'figure']):\n",
        "        element.decompose()\n",
        "\n",
        "    # Procesar cada párrafo conservando formato semántico\n",
        "    parrafos = []\n",
        "    for p in contenido_div.find_all('p'):\n",
        "        # Extraer todo el texto del párrafo incluyendo etiquetas de formato\n",
        "        texto_parrafo = p.get_text(' ', strip=True)  # El espacio une elementos separados\n",
        "        if texto_parrafo:\n",
        "            # Limpieza final de espacios múltiples\n",
        "            texto_parrafo = ' '.join(texto_parrafo.split())\n",
        "            parrafos.append(texto_parrafo)\n",
        "\n",
        "    contenido = '\\n\\n'.join(parrafos)\n",
        "else:\n",
        "    contenido = \"Contenido no encontrado\"\n",
        "\n",
        "# Resultados\n",
        "print(f\"Título: {titulo}\")\n",
        "print(f\"Fecha: {fecha_formateada}\")\n",
        "print(f\"Autor: {autor}\")\n",
        "print(\"\\n--- CONTENIDO ---\\n\")\n",
        "print(contenido)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yAi4JbyRb8XB",
        "outputId": "a01dffe9-981d-445d-e1a3-069f7bc1139c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Procesando: https://web.archive.org/web/20180101101206/https://www.elespectador.com/opinion/cesar-gaviria-y-cesar-mondragon-columna-731046\n",
            "Procesando: https://web.archive.org/web/20180101101206/https://www.elespectador.com/opinion/mas-recomendaciones-de-fin-de-ano-columna-731038\n",
            "Procesando: https://web.archive.org/web/20180127005204/https://www.elespectador.com/opinion/adios-al-papel-y-la-tinta-bienvenida-la-tecnologia-al-servicio-de-los-ciudadanos-columna-735529\n",
            "Procesando: https://web.archive.org/web/20180101101206/https://www.elespectador.com/opinion/sobre-la-construccion-de-paz-y-el-indulto-fujimori-columna-730787\n",
            "Procesando: https://web.archive.org/web/20180102104221/https://www.elespectador.com/opinion/fajardo-para-nada-tibio-columna-731305\n",
            "Procesando: https://web.archive.org/web/20180102104221/https://www.elespectador.com/opinion/macedonia-de-norte-columna-731309\n",
            "Procesando: https://web.archive.org/web/20180102104221/https://www.elespectador.com/opinion/el-nacionalismo-segun-vargas-llosa-columna-731306\n",
            "Procesando: https://web.archive.org/web/20180102104221/https://www.elespectador.com/opinion/tiempo-sagrado-tiempo-profano-columna-731302\n",
            "Procesando: https://web.archive.org/web/20180102104221/https://www.elespectador.com/opinion/la-rebelion-de-los-bueyes-columna-731238\n",
            "Procesando: https://web.archive.org/web/20180102104221/https://www.elespectador.com/opinion/muntu-bantu-columna-731236\n",
            "Procesando: https://web.archive.org/web/20180102104221/https://www.elespectador.com/opinion/2017-i-columna-731307\n",
            "Procesando: https://web.archive.org/web/20180102104221/https://www.elespectador.com/opinion/lo-que-no-cambiara-en-el-2018-columna-731237\n"
          ]
        }
      ],
      "source": [
        "# Usamos with para que el archivo se cierre automáticamente\n",
        "with open(\"urls.txt\", \"r\") as f:\n",
        "    # Leemos todas las líneas y filtramos las vacías\n",
        "    urls = [line.strip() for line in f if line.strip()]\n",
        "\n",
        "# Eliminamos duplicados\n",
        "urls = list(dict.fromkeys(urls))\n",
        "\n",
        "# Diccionario para meses en español\n",
        "meses = {\n",
        "    \"Ene\": \"enero\", \"Feb\": \"febrero\", \"Mar\": \"marzo\", \"Abr\": \"abril\",\n",
        "    \"May\": \"mayo\", \"Jun\": \"junio\", \"Jul\": \"julio\", \"Ago\": \"agosto\",\n",
        "    \"Sep\": \"septiembre\", \"Oct\": \"octubre\", \"Nov\": \"noviembre\", \"Dic\": \"diciembre\"\n",
        "}\n",
        "\n",
        "# Lista para almacenar todos los resultados\n",
        "datos = []\n",
        "n = 0\n",
        "for url in urls:\n",
        "    try:\n",
        "        print(f\"Procesando: {url}\")\n",
        "        response = requests.get(url, timeout=10)\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # 1. Extraer título\n",
        "        titulo = soup.find('h1').get_text(strip=True) if soup.find('h1') else \"Título no encontrado\"\n",
        "\n",
        "        # 2. Extraer fecha formateada\n",
        "        fecha_element = soup.find('div', class_='node-post-date')\n",
        "        if fecha_element:\n",
        "            fecha_texto = fecha_element.get_text(strip=True).split(' - ')[0]\n",
        "            try:\n",
        "                dia, mes_abrev, anio = fecha_texto.split()\n",
        "                fecha_formateada = f\"{dia} de {meses[mes_abrev]} de {anio}\"\n",
        "            except:\n",
        "                fecha_formateada = fecha_texto\n",
        "        else:\n",
        "            fecha_formateada = \"Fecha no encontrada\"\n",
        "\n",
        "        # 3. Extraer autor\n",
        "        autor_element = soup.find('span', class_='by')\n",
        "        if autor_element:\n",
        "            autor = autor_element.next_sibling.strip() if autor_element.next_sibling else \"Autor no encontrado\"\n",
        "        else:\n",
        "            autor = \"Autor no encontrado\"\n",
        "\n",
        "        # 4. Extraer contenido limpio con párrafos (versión mejorada)\n",
        "        contenido_div = soup.find('div', class_='node-body')\n",
        "        if contenido_div:\n",
        "            # Primero eliminar el div no deseado si existe\n",
        "            info_node = contenido_div.find('div', class_='info_node_hide')\n",
        "            if info_node:\n",
        "                info_node.decompose()  # Esto elimina completamente el div y su contenido\n",
        "\n",
        "            # Eliminar solo elementos no deseados (scripts, iframes, etc.)\n",
        "            for element in contenido_div(['script', 'style', 'iframe', 'img', 'figure']):\n",
        "                element.decompose()\n",
        "\n",
        "            # Procesar cada párrafo conservando formato semántico\n",
        "            parrafos = []\n",
        "            for p in contenido_div.find_all('p'):\n",
        "                # Extraer todo el texto del párrafo incluyendo etiquetas de formato\n",
        "                texto_parrafo = p.get_text(' ', strip=True)  # El espacio une elementos separados\n",
        "                if texto_parrafo:\n",
        "                    # Limpieza final de espacios múltiples\n",
        "                    texto_parrafo = ' '.join(texto_parrafo.split())\n",
        "                    parrafos.append(texto_parrafo)\n",
        "\n",
        "            contenido = '\\n\\n'.join(parrafos)\n",
        "        else:\n",
        "            contenido = \"Contenido no encontrado\"\n",
        "\n",
        "        # Agregar a la lista de datos\n",
        "        datos.append({\n",
        "            'Autor': autor,\n",
        "            'Fecha': fecha_formateada,\n",
        "            'Título': titulo,\n",
        "            'Contenido': contenido,\n",
        "            'URL': url\n",
        "        })\n",
        "        #time.sleep(1)\n",
        "    except Exception as e:\n",
        "        print(f\"Error procesando {url}: {str(e)}\")\n",
        "        datos.append({\n",
        "            'Autor': f\"Error: {str(e)}\",\n",
        "            'Fecha': \"\",\n",
        "            'Título': \"\",\n",
        "            'Contenido': \"\",\n",
        "            'URL': url\n",
        "        })\n",
        "    n = n + 1\n",
        "    if n % 20 == 0 and n < len(urls):\n",
        "        print(f\"Procesados: {n} de {len(urls)}\")\n",
        "        time.sleep(200)\n",
        "\n",
        "\n",
        "\n",
        "# Crear DataFrame y guardar como CSV\n",
        "df = pd.DataFrame(datos)\n",
        "\n",
        "# Ordenar columnas\n",
        "column_order = ['Autor', 'Fecha', 'Título', 'Contenido', 'URL']\n",
        "df = df[column_order]\n",
        "\n",
        "# Crear archivo Excel\n",
        "nombre_archivo = \"resultados_articulos.xlsx\"\n",
        "with pd.ExcelWriter(nombre_archivo, engine='openpyxl') as writer:\n",
        "    df.to_excel(writer, index=False, sheet_name='Artículos')\n",
        "\n",
        "    # Ajustar el ancho de las columnas\n",
        "    worksheet = writer.sheets['Artículos']\n",
        "    worksheet.column_dimensions['A'].width = 25  # Autor\n",
        "    worksheet.column_dimensions['B'].width = 20  # Fecha\n",
        "    worksheet.column_dimensions['C'].width = 40  # Título\n",
        "    worksheet.column_dimensions['D'].width = 80  # Contenido\n",
        "    worksheet.column_dimensions['E'].width = 60  # URL\n",
        "\n",
        "print(f\"\\nProceso completado. Resultados guardados en {nombre_archivo}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: requests-html in c:\\users\\karen\\anaconda3\\lib\\site-packages (0.10.0)\n",
            "Requirement already satisfied: requests in c:\\users\\karen\\anaconda3\\lib\\site-packages (from requests-html) (2.32.3)\n",
            "Requirement already satisfied: pyquery in c:\\users\\karen\\anaconda3\\lib\\site-packages (from requests-html) (2.0.1)\n",
            "Requirement already satisfied: fake-useragent in c:\\users\\karen\\anaconda3\\lib\\site-packages (from requests-html) (2.2.0)\n",
            "Requirement already satisfied: parse in c:\\users\\karen\\anaconda3\\lib\\site-packages (from requests-html) (1.20.2)\n",
            "Requirement already satisfied: bs4 in c:\\users\\karen\\anaconda3\\lib\\site-packages (from requests-html) (0.0.2)\n",
            "Requirement already satisfied: w3lib in c:\\users\\karen\\anaconda3\\lib\\site-packages (from requests-html) (2.1.2)\n",
            "Requirement already satisfied: pyppeteer>=0.0.14 in c:\\users\\karen\\anaconda3\\lib\\site-packages (from requests-html) (2.0.0)\n",
            "Requirement already satisfied: appdirs<2.0.0,>=1.4.3 in c:\\users\\karen\\anaconda3\\lib\\site-packages (from pyppeteer>=0.0.14->requests-html) (1.4.4)\n",
            "Requirement already satisfied: certifi>=2023 in c:\\users\\karen\\anaconda3\\lib\\site-packages (from pyppeteer>=0.0.14->requests-html) (2025.1.31)\n",
            "Requirement already satisfied: importlib-metadata>=1.4 in c:\\users\\karen\\anaconda3\\lib\\site-packages (from pyppeteer>=0.0.14->requests-html) (7.0.1)\n",
            "Requirement already satisfied: pyee<12.0.0,>=11.0.0 in c:\\users\\karen\\anaconda3\\lib\\site-packages (from pyppeteer>=0.0.14->requests-html) (11.1.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.42.1 in c:\\users\\karen\\anaconda3\\lib\\site-packages (from pyppeteer>=0.0.14->requests-html) (4.66.5)\n",
            "Requirement already satisfied: urllib3<2.0.0,>=1.25.8 in c:\\users\\karen\\anaconda3\\lib\\site-packages (from pyppeteer>=0.0.14->requests-html) (1.26.20)\n",
            "Requirement already satisfied: websockets<11.0,>=10.0 in c:\\users\\karen\\anaconda3\\lib\\site-packages (from pyppeteer>=0.0.14->requests-html) (10.4)\n",
            "Requirement already satisfied: beautifulsoup4 in c:\\users\\karen\\anaconda3\\lib\\site-packages (from bs4->requests-html) (4.12.3)\n",
            "Requirement already satisfied: lxml>=2.1 in c:\\users\\karen\\anaconda3\\lib\\site-packages (from pyquery->requests-html) (5.2.1)\n",
            "Requirement already satisfied: cssselect>=1.2.0 in c:\\users\\karen\\anaconda3\\lib\\site-packages (from pyquery->requests-html) (1.2.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\karen\\anaconda3\\lib\\site-packages (from requests->requests-html) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\karen\\anaconda3\\lib\\site-packages (from requests->requests-html) (3.7)\n",
            "Requirement already satisfied: zipp>=0.5 in c:\\users\\karen\\anaconda3\\lib\\site-packages (from importlib-metadata>=1.4->pyppeteer>=0.0.14->requests-html) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions in c:\\users\\karen\\anaconda3\\lib\\site-packages (from pyee<12.0.0,>=11.0.0->pyppeteer>=0.0.14->requests-html) (4.11.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\karen\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.42.1->pyppeteer>=0.0.14->requests-html) (0.4.6)\n",
            "Requirement already satisfied: soupsieve>1.2 in c:\\users\\karen\\anaconda3\\lib\\site-packages (from beautifulsoup4->bs4->requests-html) (2.5)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install requests-html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting requests-html\n",
            "  Using cached requests_html-0.10.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting requests (from requests-html)\n",
            "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting pyquery (from requests-html)\n",
            "  Using cached pyquery-2.0.1-py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting fake-useragent (from requests-html)\n",
            "  Using cached fake_useragent-2.2.0-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting parse (from requests-html)\n",
            "  Using cached parse-1.20.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting bs4 (from requests-html)\n",
            "  Using cached bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\n",
            "Collecting w3lib (from requests-html)\n",
            "  Downloading w3lib-2.3.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting pyppeteer>=0.0.14 (from requests-html)\n",
            "  Using cached pyppeteer-2.0.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting appdirs<2.0.0,>=1.4.3 (from pyppeteer>=0.0.14->requests-html)\n",
            "  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting certifi>=2023 (from pyppeteer>=0.0.14->requests-html)\n",
            "  Downloading certifi-2025.4.26-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting importlib-metadata>=1.4 (from pyppeteer>=0.0.14->requests-html)\n",
            "  Downloading importlib_metadata-8.7.0-py3-none-any.whl.metadata (4.8 kB)\n",
            "Collecting pyee<12.0.0,>=11.0.0 (from pyppeteer>=0.0.14->requests-html)\n",
            "  Using cached pyee-11.1.1-py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting tqdm<5.0.0,>=4.42.1 (from pyppeteer>=0.0.14->requests-html)\n",
            "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
            "Collecting urllib3<2.0.0,>=1.25.8 (from pyppeteer>=0.0.14->requests-html)\n",
            "  Using cached urllib3-1.26.20-py2.py3-none-any.whl.metadata (50 kB)\n",
            "Collecting websockets<11.0,>=10.0 (from pyppeteer>=0.0.14->requests-html)\n",
            "  Using cached websockets-10.4-cp312-cp312-win_amd64.whl\n",
            "Collecting beautifulsoup4 (from bs4->requests-html)\n",
            "  Downloading beautifulsoup4-4.13.4-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting lxml>=2.1 (from pyquery->requests-html)\n",
            "  Downloading lxml-5.4.0-cp312-cp312-win_amd64.whl.metadata (3.6 kB)\n",
            "Collecting cssselect>=1.2.0 (from pyquery->requests-html)\n",
            "  Downloading cssselect-1.3.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting charset-normalizer<4,>=2 (from requests->requests-html)\n",
            "  Downloading charset_normalizer-3.4.2-cp312-cp312-win_amd64.whl.metadata (36 kB)\n",
            "Collecting idna<4,>=2.5 (from requests->requests-html)\n",
            "  Downloading idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting zipp>=3.20 (from importlib-metadata>=1.4->pyppeteer>=0.0.14->requests-html)\n",
            "  Downloading zipp-3.21.0-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting typing-extensions (from pyee<12.0.0,>=11.0.0->pyppeteer>=0.0.14->requests-html)\n",
            "  Downloading typing_extensions-4.13.2-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting colorama (from tqdm<5.0.0,>=4.42.1->pyppeteer>=0.0.14->requests-html)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Collecting soupsieve>1.2 (from beautifulsoup4->bs4->requests-html)\n",
            "  Downloading soupsieve-2.7-py3-none-any.whl.metadata (4.6 kB)\n",
            "Using cached requests_html-0.10.0-py3-none-any.whl (13 kB)\n",
            "Using cached pyppeteer-2.0.0-py3-none-any.whl (82 kB)\n",
            "Using cached bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
            "Using cached fake_useragent-2.2.0-py3-none-any.whl (161 kB)\n",
            "Using cached parse-1.20.2-py2.py3-none-any.whl (20 kB)\n",
            "Using cached pyquery-2.0.1-py3-none-any.whl (22 kB)\n",
            "Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "Downloading w3lib-2.3.1-py3-none-any.whl (21 kB)\n",
            "Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
            "Downloading certifi-2025.4.26-py3-none-any.whl (159 kB)\n",
            "Downloading charset_normalizer-3.4.2-cp312-cp312-win_amd64.whl (105 kB)\n",
            "Downloading cssselect-1.3.0-py3-none-any.whl (18 kB)\n",
            "Downloading idna-3.10-py3-none-any.whl (70 kB)\n",
            "Downloading importlib_metadata-8.7.0-py3-none-any.whl (27 kB)\n",
            "Downloading lxml-5.4.0-cp312-cp312-win_amd64.whl (3.8 MB)\n",
            "   ---------------------------------------- 0.0/3.8 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/3.8 MB ? eta -:--:--\n",
            "   -- ------------------------------------- 0.3/3.8 MB ? eta -:--:--\n",
            "   -------- ------------------------------- 0.8/3.8 MB 1.3 MB/s eta 0:00:03\n",
            "   ---------- ----------------------------- 1.0/3.8 MB 1.3 MB/s eta 0:00:03\n",
            "   ------------- -------------------------- 1.3/3.8 MB 1.3 MB/s eta 0:00:02\n",
            "   ---------------- ----------------------- 1.6/3.8 MB 1.3 MB/s eta 0:00:02\n",
            "   ------------------- -------------------- 1.8/3.8 MB 1.3 MB/s eta 0:00:02\n",
            "   --------------------- ------------------ 2.1/3.8 MB 1.4 MB/s eta 0:00:02\n",
            "   ------------------------ --------------- 2.4/3.8 MB 1.3 MB/s eta 0:00:02\n",
            "   --------------------------- ------------ 2.6/3.8 MB 1.3 MB/s eta 0:00:01\n",
            "   ------------------------------ --------- 2.9/3.8 MB 1.3 MB/s eta 0:00:01\n",
            "   -------------------------------- ------- 3.1/3.8 MB 1.3 MB/s eta 0:00:01\n",
            "   -------------------------------------- - 3.7/3.8 MB 1.4 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 3.8/3.8 MB 1.4 MB/s eta 0:00:00\n",
            "Using cached pyee-11.1.1-py3-none-any.whl (15 kB)\n",
            "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
            "Using cached urllib3-1.26.20-py2.py3-none-any.whl (144 kB)\n",
            "Downloading beautifulsoup4-4.13.4-py3-none-any.whl (187 kB)\n",
            "Downloading soupsieve-2.7-py3-none-any.whl (36 kB)\n",
            "Downloading typing_extensions-4.13.2-py3-none-any.whl (45 kB)\n",
            "Downloading zipp-3.21.0-py3-none-any.whl (9.6 kB)\n",
            "Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Installing collected packages: parse, appdirs, zipp, websockets, w3lib, urllib3, typing-extensions, soupsieve, lxml, idna, fake-useragent, cssselect, colorama, charset-normalizer, certifi, tqdm, requests, pyquery, pyee, importlib-metadata, beautifulsoup4, pyppeteer, bs4, requests-html\n",
            "  Attempting uninstall: parse\n",
            "    Found existing installation: parse 1.20.2\n",
            "    Uninstalling parse-1.20.2:\n",
            "      Successfully uninstalled parse-1.20.2\n",
            "  Attempting uninstall: appdirs\n",
            "    Found existing installation: appdirs 1.4.4\n",
            "    Uninstalling appdirs-1.4.4:\n",
            "      Successfully uninstalled appdirs-1.4.4\n",
            "  Attempting uninstall: zipp\n",
            "    Found existing installation: zipp 3.17.0\n",
            "    Uninstalling zipp-3.17.0:\n",
            "      Successfully uninstalled zipp-3.17.0\n",
            "  Attempting uninstall: websockets\n",
            "    Found existing installation: websockets 10.4\n",
            "    Uninstalling websockets-10.4:\n",
            "      Successfully uninstalled websockets-10.4\n",
            "  Attempting uninstall: w3lib\n",
            "    Found existing installation: w3lib 2.1.2\n",
            "    Uninstalling w3lib-2.1.2:\n",
            "      Successfully uninstalled w3lib-2.1.2\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.26.20\n",
            "    Uninstalling urllib3-1.26.20:\n",
            "      Successfully uninstalled urllib3-1.26.20\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.11.0\n",
            "    Uninstalling typing_extensions-4.11.0:\n",
            "      Successfully uninstalled typing_extensions-4.11.0\n",
            "  Attempting uninstall: soupsieve\n",
            "    Found existing installation: soupsieve 2.5\n",
            "    Uninstalling soupsieve-2.5:\n",
            "      Successfully uninstalled soupsieve-2.5\n",
            "  Attempting uninstall: lxml\n",
            "    Found existing installation: lxml 5.2.1\n",
            "    Uninstalling lxml-5.2.1:\n",
            "      Successfully uninstalled lxml-5.2.1\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.7\n",
            "    Uninstalling idna-3.7:\n",
            "      Successfully uninstalled idna-3.7\n",
            "  Attempting uninstall: fake-useragent\n",
            "    Found existing installation: fake-useragent 2.2.0\n",
            "    Uninstalling fake-useragent-2.2.0:\n",
            "      Successfully uninstalled fake-useragent-2.2.0\n",
            "  Attempting uninstall: cssselect\n",
            "    Found existing installation: cssselect 1.2.0\n",
            "    Uninstalling cssselect-1.2.0:\n",
            "      Successfully uninstalled cssselect-1.2.0\n",
            "  Attempting uninstall: colorama\n",
            "    Found existing installation: colorama 0.4.6\n",
            "    Uninstalling colorama-0.4.6:\n",
            "      Successfully uninstalled colorama-0.4.6\n",
            "  Attempting uninstall: charset-normalizer\n",
            "    Found existing installation: charset-normalizer 3.3.2\n",
            "    Uninstalling charset-normalizer-3.3.2:\n",
            "      Successfully uninstalled charset-normalizer-3.3.2\n",
            "  Attempting uninstall: certifi\n",
            "    Found existing installation: certifi 2025.1.31\n",
            "    Uninstalling certifi-2025.1.31:\n",
            "      Successfully uninstalled certifi-2025.1.31\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.66.5\n",
            "    Uninstalling tqdm-4.66.5:\n",
            "      Successfully uninstalled tqdm-4.66.5\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.32.3\n",
            "    Uninstalling requests-2.32.3:\n",
            "      Successfully uninstalled requests-2.32.3\n",
            "  Attempting uninstall: pyquery\n",
            "    Found existing installation: pyquery 2.0.1\n",
            "    Uninstalling pyquery-2.0.1:\n",
            "      Successfully uninstalled pyquery-2.0.1\n",
            "  Attempting uninstall: pyee\n",
            "    Found existing installation: pyee 11.1.1\n",
            "    Uninstalling pyee-11.1.1:\n",
            "      Successfully uninstalled pyee-11.1.1\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib-metadata 7.0.1\n",
            "    Uninstalling importlib-metadata-7.0.1:\n",
            "      Successfully uninstalled importlib-metadata-7.0.1\n",
            "  Attempting uninstall: beautifulsoup4\n",
            "    Found existing installation: beautifulsoup4 4.12.3\n",
            "    Uninstalling beautifulsoup4-4.12.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.12.3\n",
            "  Attempting uninstall: pyppeteer\n",
            "    Found existing installation: pyppeteer 2.0.0\n",
            "    Uninstalling pyppeteer-2.0.0:\n",
            "      Successfully uninstalled pyppeteer-2.0.0\n",
            "  Attempting uninstall: bs4\n",
            "    Found existing installation: bs4 0.0.2\n",
            "    Uninstalling bs4-0.0.2:\n",
            "      Successfully uninstalled bs4-0.0.2\n",
            "  Attempting uninstall: requests-html\n",
            "    Found existing installation: requests-html 0.10.0\n",
            "    Uninstalling requests-html-0.10.0:\n",
            "      Successfully uninstalled requests-html-0.10.0\n",
            "Successfully installed appdirs-1.4.4 beautifulsoup4-4.13.4 bs4-0.0.2 certifi-2025.4.26 charset-normalizer-3.4.2 colorama-0.4.6 cssselect-1.3.0 fake-useragent-2.2.0 idna-3.10 importlib-metadata-8.7.0 lxml-5.4.0 parse-1.20.2 pyee-11.1.1 pyppeteer-2.0.0 pyquery-2.0.1 requests-2.32.3 requests-html-0.10.0 soupsieve-2.7 tqdm-4.67.1 typing-extensions-4.13.2 urllib3-1.26.20 w3lib-2.3.1 websockets-10.4 zipp-3.21.0\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\karen\\anaconda3\\Lib\\site-packages\\~xml'.\n",
            "  You can safely remove it manually.\n",
            "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "aext-assistant-server 4.1.0 requires anaconda-cloud-auth>=0.7.1, which is not installed.\n",
            "conda-repo-cli 1.0.114 requires urllib3>=2.2.2, but you have urllib3 1.26.20 which is incompatible.\n"
          ]
        }
      ],
      "source": [
        "pip install --force-reinstall requests-html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Procesando 1/1: https://web.archive.org/web/20200609164031mp_/https://www.elespectador.com/opinion/interrumpir-el-olvido/\n",
            "Error en https://web.archive.org/web/20200609164031mp_/https://www.elespectador.com/opinion/interrumpir-el-olvido/: Cannot use HTMLSession within an existing event loop. Use AsyncHTMLSession instead.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Título</th>\n",
              "      <th>Autor</th>\n",
              "      <th>Contenido</th>\n",
              "      <th>URL</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Error: Cannot use HTMLSession within an existi...</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>https://web.archive.org/web/20200609164031mp_/...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              Título Autor Contenido  \\\n",
              "0  Error: Cannot use HTMLSession within an existi...                   \n",
              "\n",
              "                                                 URL  \n",
              "0  https://web.archive.org/web/20200609164031mp_/...  "
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from requests_html import HTMLSession\n",
        "import pandas as pd\n",
        "from time import sleep\n",
        "\n",
        "# Iniciar sesión\n",
        "session = HTMLSession()\n",
        "\n",
        "def scrape_page(url):\n",
        "    try:\n",
        "        # Hacer la petición y renderizar JavaScript\n",
        "        r = session.get(url)\n",
        "        r.html.render(sleep=2, timeout=20)  # sleep espera 2 segundos después de cargar\n",
        "        \n",
        "        # Extraer datos (ajusta estos selectores según la nueva página)\n",
        "        titulo = r.html.find('.Article-Title', first=True).text if r.html.find('.Article-Title') else \"No encontrado\"\n",
        "        \n",
        "        # Buscar autor - ajusta el selector según la nueva estructura\n",
        "        autor = r.html.find('.author-name', first=True).text if r.html.find('.author-name') else \"Anónimo\"\n",
        "        \n",
        "        # Extraer contenido - ajusta el selector\n",
        "        parrafos = [p.text for p in r.html.find('article p')] if r.html.find('article p') else []\n",
        "        contenido = '\\n\\n'.join(parrafos) if parrafos else \"Contenido no disponible\"\n",
        "        \n",
        "        return {\n",
        "            'Título': titulo,\n",
        "            'Autor': autor,\n",
        "            'Contenido': contenido,\n",
        "            'URL': url\n",
        "        }\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Error en {url}: {str(e)}\")\n",
        "        return {\n",
        "            'Título': f\"Error: {str(e)}\",\n",
        "            'Autor': \"\",\n",
        "            'Contenido': \"\",\n",
        "            'URL': url\n",
        "        }\n",
        "\n",
        "# Lista de URLs (ejemplo)\n",
        "urls = [\n",
        "    \"https://web.archive.org/web/20200609164031mp_/https://www.elespectador.com/opinion/interrumpir-el-olvido/\",\n",
        "]\n",
        "\n",
        "# Procesar todas las URLs\n",
        "datos = []\n",
        "for i, url in enumerate(urls, 1):\n",
        "    print(f\"Procesando {i}/{len(urls)}: {url}\")\n",
        "    datos.append(scrape_page(url))\n",
        "    sleep(1)  # Pausa entre solicitudes\n",
        "\n",
        "# Guardar en Excel\n",
        "df = pd.DataFrame(datos)\n",
        "df\n",
        "#df.to_excel('articulos_elespectador.xlsx', index=False)\n",
        "#print(\"Scraping completado. Datos guardados en articulos_elespectador.xlsx\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Nuevo scrapping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Error sending stats to Plausible: error sending request for url (https://plausible.io/api/event)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Procesando (1/1): https://web.archive.org/web/20200609163417mp_/https://www.elespectador.com/opinion/educacion-pandemia-mas-inequidad/\n",
            "Contenido incompleto, usando Selenium...\n",
            "Falló método rápido, intentando con Selenium: Contenido requiere JavaScript\n",
            "Error con Selenium en https://web.archive.org/web/20200609163417mp_/https://www.elespectador.com/opinion/educacion-pandemia-mas-inequidad/: module 'selenium.webdriver.support.expected_conditions' has no attribute 'or_'\n",
            "\n",
            "Proceso completado. Resultados guardados en resultados_articulos.xlsx\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "import time\n",
        "import re\n",
        "\n",
        "def setup_driver():\n",
        "    \"\"\"Configura el driver de Chrome optimizado\"\"\"\n",
        "    chrome_options = Options()\n",
        "    chrome_options.add_argument(\"--headless\")\n",
        "    chrome_options.add_argument(\"--disable-gpu\")\n",
        "    chrome_options.add_argument(\"--no-sandbox\")\n",
        "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
        "    chrome_options.add_argument(\"--blink-settings=imagesEnabled=false\")\n",
        "    chrome_options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\")\n",
        "    driver = webdriver.Chrome(options=chrome_options)\n",
        "    driver.set_page_load_timeout(30)\n",
        "    return driver\n",
        "\n",
        "def parse_date(date_str):\n",
        "    \"\"\"Convierte fechas al formato '1 de febrero de 2020'\"\"\"\n",
        "    try:\n",
        "        months_es = {\n",
        "            'Jan': 'enero', 'Feb': 'febrero', 'Mar': 'marzo', 'Apr': 'abril',\n",
        "            'May': 'mayo', 'Jun': 'junio', 'Jul': 'julio', 'Aug': 'agosto',\n",
        "            'Sep': 'septiembre', 'Oct': 'octubre', 'Nov': 'noviembre', 'Dec': 'diciembre',\n",
        "            'January': 'enero', 'February': 'febrero', 'March': 'marzo', 'April': 'abril',\n",
        "            'June': 'junio', 'July': 'julio', 'August': 'agosto', 'September': 'septiembre',\n",
        "            'October': 'octubre', 'November': 'noviembre', 'December': 'diciembre'\n",
        "        }\n",
        "        \n",
        "        # Intenta parsear desde formato ISO (2020-06-01T01:16:11.578Z)\n",
        "        if 'T' in date_str:\n",
        "            dt = datetime.fromisoformat(date_str.replace('Z', '+00:00'))\n",
        "            day = dt.day\n",
        "            month = months_es[dt.strftime('%B')]\n",
        "            year = dt.year\n",
        "            return f\"{day} de {month} de {year}\"\n",
        "        \n",
        "        # Intenta parsear desde formato '1 Jun 2020'\n",
        "        parts = re.split(r'[\\s,]+', date_str.strip())\n",
        "        if len(parts) >= 3:\n",
        "            day = parts[0]\n",
        "            month_abbr = parts[1]\n",
        "            month = months_es.get(month_abbr, month_abbr)\n",
        "            year = parts[2]\n",
        "            return f\"{day} de {month} de {year}\"\n",
        "            \n",
        "        return date_str\n",
        "    except Exception as e:\n",
        "        print(f\"Error parseando fecha '{date_str}': {str(e)}\")\n",
        "        return date_str\n",
        "\n",
        "def extract_with_selenium(driver, url):\n",
        "    \"\"\"Extrae datos usando Selenium para contenido dinámico\"\"\"\n",
        "    try:\n",
        "        driver.get(url)\n",
        "        \n",
        "        # Espera inteligente con timeout extendido\n",
        "        WebDriverWait(driver, 15).until(\n",
        "            EC.or_(\n",
        "                EC.presence_of_element_located((By.CSS_SELECTOR, \".Article-Title\")),\n",
        "                EC.presence_of_element_located((By.CSS_SELECTOR, \".font--secondary\"))\n",
        "            )\n",
        "        )\n",
        "\n",
        "        # Extracción optimizada con JavaScript\n",
        "        article_data = driver.execute_script(\"\"\"\n",
        "            const getText = (selector) => {\n",
        "                const el = document.querySelector(selector);\n",
        "                return el ? el.innerText.trim() : '';\n",
        "            };\n",
        "            \n",
        "            const getContent = () => {\n",
        "                const contentEl = document.querySelector('.font--secondary') || \n",
        "                                 document.querySelector('article');\n",
        "                if (!contentEl) return '';\n",
        "                \n",
        "                return Array.from(contentEl.querySelectorAll('.font--secondary'))\n",
        "                    .map(.font--secondary => .font--secondary.innerText.trim())\n",
        "                    .filter(t => t.length > 0)\n",
        "                    .join('\\\\n\\\\n');\n",
        "            };\n",
        "            \n",
        "            return {\n",
        "                title: getText('.Article-Title'),\n",
        "                author: document.querySelector('.Article-Author')?.content || '',\n",
        "                date: document.querySelector('.Article-Time')?.content || \n",
        "                      document.querySelector('meta[name=\"article:published_time\"]')?.content ||\n",
        "                      document.querySelector('time.Article-Time')?.innerText.split('-')[0].trim() || '',\n",
        "                content: getContent()\n",
        "            };\n",
        "        \"\"\")\n",
        "        \n",
        "        return {\n",
        "            'Autor': article_data.get('author', 'Autor no encontrado'),\n",
        "            'Fecha': parse_date(article_data.get('date', 'Fecha no encontrada')),\n",
        "            'Título': article_data.get('title', 'No encontrado'),\n",
        "            'Contenido': article_data.get('content', 'Contenido no disponible'),\n",
        "            'Vínculo': url\n",
        "        }\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Error con Selenium en {url}: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "def process_urls(urls_file, output_file):\n",
        "    \"\"\"Procesa múltiples URLs desde archivo\"\"\"\n",
        "    with open(urls_file, 'r', encoding='utf-8') as f:\n",
        "        urls = [line.strip() for line in f if line.strip()]\n",
        "    \n",
        "    driver = setup_driver()\n",
        "    results = []\n",
        "    \n",
        "    for i, url in enumerate(urls, 1):\n",
        "        print(f\"Procesando ({i}/{len(urls)}): {url}\")\n",
        "        \n",
        "        try:\n",
        "            # Primero intenta con requests (más rápido)\n",
        "            headers = {\n",
        "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "            }\n",
        "            response = requests.get(url, headers=headers, timeout=15)\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "            \n",
        "            # Verifica si el contenido está completo\n",
        "            if not soup.find('div', class_='Article-Content'):\n",
        "                print(\"Contenido incompleto, usando Selenium...\")\n",
        "                raise ValueError(\"Contenido requiere JavaScript\")\n",
        "            \n",
        "            # Extracción con BeautifulSoup\n",
        "            title = soup.find('h1', class_='Article-Title')\n",
        "            author = soup.find('meta', {'name': 'cXenseParse:author'})\n",
        "            date = (soup.find('meta', {'name': 'article:modified_time'}) or \n",
        "                   soup.find('meta', {'name': 'article:published_time'}) or\n",
        "                   soup.find('time', class_='Article-Time'))\n",
        "            \n",
        "            content_div = soup.find('div', class_='Article-Content')\n",
        "            paragraphs = []\n",
        "            if content_div:\n",
        "                for p in content_div.find_all('p'):\n",
        "                    paragraphs.append(p.get_text().strip())\n",
        "            \n",
        "            date_value = ''\n",
        "            if date:\n",
        "                if date.get('content'):\n",
        "                    date_value = date['content']\n",
        "                else:\n",
        "                    date_value = date.get_text().split('-')[0].strip()\n",
        "            \n",
        "            result = {\n",
        "                'Autor': author['content'] if author else 'Autor no encontrado',\n",
        "                'Fecha': parse_date(date_value) if date_value else 'Fecha no encontrada',\n",
        "                'Título': title.get_text().strip() if title else 'No encontrado',\n",
        "                'Contenido': '\\n\\n'.join(paragraphs) if paragraphs else 'Contenido no disponible',\n",
        "                'Vínculo': url\n",
        "            }\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"Falló método rápido, intentando con Selenium: {str(e)}\")\n",
        "            result = extract_with_selenium(driver, url)\n",
        "            if not result:\n",
        "                result = {\n",
        "                    'Autor': 'Error al extraer autor',\n",
        "                    'Fecha': 'Error al extraer fecha',\n",
        "                    'Título': 'Error al extraer título',\n",
        "                    'Contenido': 'Error al extraer contenido',\n",
        "                    'Vínculo': url\n",
        "                }\n",
        "        \n",
        "        results.append(result)\n",
        "        time.sleep(2)  # Intervalo entre requests\n",
        "    \n",
        "    driver.quit()\n",
        "    \n",
        "    # Guardar resultados en el orden solicitado\n",
        "    df = pd.DataFrame(results)[['Autor', 'Fecha', 'Título', 'Contenido', 'Vínculo']]\n",
        "    \n",
        "    # Asegurar formato correcto en Excel\n",
        "    writer = pd.ExcelWriter(output_file, engine='xlsxwriter')\n",
        "    df.to_excel(writer, index=False, sheet_name='Artículos')\n",
        "    \n",
        "    # Ajustar el ancho de las columnas\n",
        "    worksheet = writer.sheets['Artículos']\n",
        "    worksheet.set_column('A:A', 30)  # Autor\n",
        "    worksheet.set_column('B:B', 25)  # Fecha\n",
        "    worksheet.set_column('C:C', 50)  # Título\n",
        "    worksheet.set_column('D:D', 100) # Contenido\n",
        "    worksheet.set_column('E:E', 60)  # Vínculo\n",
        "    \n",
        "    writer.close()\n",
        "    print(f\"\\nProceso completado. Resultados guardados en {output_file}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    process_urls('urls.txt', 'resultados_articulos.xlsx')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Para extraer links"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Se encontraron 0 enlaces de columnas válidos:\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "fecha = \"20200616194558\"\n",
        "url = \"https://web.archive.org/web/\" + fecha + \"/https://www.elespectador.com/opinion/\"\n",
        "inicio = \"/web/\" + fecha + \"mp_/https://www.elespectador.com/\"\n",
        "\n",
        "try:\n",
        "    response = requests.get(url)\n",
        "    response.raise_for_status()\n",
        "except requests.exceptions.RequestException as e:\n",
        "    print(f\"Error al hacer la petición HTTP: {e}\")\n",
        "    exit()\n",
        "\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "column_items = soup.find_all('li', class_='views-row')\n",
        "\n",
        "column_links = []\n",
        "base_url = \"https://web.archive.org\"\n",
        "\n",
        "excluded_authors = [\n",
        "    \"Columnista invitado EE\",\n",
        "    \"Cartas de los lectores\",\n",
        "    \"Las igualadas\",\n",
        "    \"La Pulla\",\n",
        "    \"Antieditorial\",\n",
        "    \"Columna del lector\",\n",
        "    \"La Puesverdad\"\n",
        "]\n",
        "\n",
        "for item in column_items:\n",
        "    # Obtener el autor de manera más robusta\n",
        "    author = \"\"\n",
        "    author_div = item.find('div', class_='views-field-field-columnist')\n",
        "    if author_div:\n",
        "        author_link = author_div.find('a')\n",
        "        if author_link:\n",
        "            author = author_link.get_text(strip=True)\n",
        "        else:\n",
        "            author = author_div.get_text(strip=True).replace(\"Por: \", \"\")\n",
        "    \n",
        "    # Verificar si el autor está en la lista de exclusiones\n",
        "    if any(excluded.lower() in author.lower() for excluded in excluded_authors):\n",
        "        continue\n",
        "    \n",
        "    # Obtener el enlace de manera más segura\n",
        "    title_div = item.find('div', class_='views-field-title')\n",
        "    if not title_div:\n",
        "        continue\n",
        "        \n",
        "    title_link = title_div.find('a')\n",
        "    if not title_link or 'href' not in title_link.attrs:\n",
        "        continue\n",
        "    \n",
        "    href = title_link['href']\n",
        "    \n",
        "    # Verificar que sea un enlace de columna válido\n",
        "\n",
        "    #if (href.startswith(inicio) and 'columna-' in href):\n",
        "    if (href.startswith(inicio) in href):\n",
        "        full_url = base_url + href if not href.startswith(base_url) else href\n",
        "        column_links.append(full_url)\n",
        "\n",
        "# Eliminar duplicados manteniendo el orden\n",
        "seen = set()\n",
        "column_links = [x for x in column_links if not (x in seen or seen.add(x))]\n",
        "\n",
        "print(f\"\\nSe encontraron {len(column_links)} enlaces de columnas válidos:\")\n",
        "for i, link in enumerate(column_links, 1):\n",
        "    print(f\"{link}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Se encontraron 0 enlaces de columnas válidos:\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import json\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "\n",
        "url = \"https://web.archive.org/web/20200618142303/https://www.elespectador.com/opinion/\"\n",
        "\n",
        "try:\n",
        "    response = requests.get(url)\n",
        "    response.raise_for_status()\n",
        "except requests.exceptions.RequestException as e:\n",
        "    print(f\"Error al hacer la petición HTTP: {e}\")\n",
        "    exit()\n",
        "\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "# Buscar el script que contiene los datos JSON\n",
        "script_tag = soup.find('script', type='application/javascript')\n",
        "if not script_tag:\n",
        "    print(\"No se encontró el script con los datos JSON\")\n",
        "    exit()\n",
        "\n",
        "# Extraer el JSON del script\n",
        "json_data = {}\n",
        "try:\n",
        "    # Buscar la variable Fusion.globalContent\n",
        "    match = re.search(r'Fusion\\.globalContent\\s*=\\s*({.*?});', script_tag.string, re.DOTALL)\n",
        "    if match:\n",
        "        json_str = match.group(1)\n",
        "        json_data = json.loads(json_str)\n",
        "except (AttributeError, json.JSONDecodeError) as e:\n",
        "    print(f\"Error al procesar los datos JSON: {e}\")\n",
        "    exit()\n",
        "\n",
        "# Lista para almacenar los enlaces válidos\n",
        "column_links = []\n",
        "base_url = \"https://web.archive.org\"\n",
        "\n",
        "# Excluir estos tipos de contenido\n",
        "excluded_types = [\n",
        "    \"Columnista invitado EE\",\n",
        "    \"Cartas de los lectores\",\n",
        "    \"Las igualadas\",\n",
        "    \"La Pulla\",\n",
        "    \"Antieditorial\",\n",
        "    \"Columna del lector\",\n",
        "    \"La Puesverdad\"\n",
        "]\n",
        "\n",
        "# Procesar los elementos de contenido\n",
        "if 'content_elements' in json_data:\n",
        "    for element in json_data['content_elements']:\n",
        "        if element.get('type') == 'story':\n",
        "            # Obtener URL canónica\n",
        "            canonical_url = element.get('canonical_url', '')\n",
        "            \n",
        "            # Verificar que sea una columna de opinión\n",
        "            if canonical_url and '/opinion/' in canonical_url:\n",
        "                # Construir URL de Wayback Machine\n",
        "                wayback_date = \"20200618142303\"  # Puedes extraer esto de la URL original si es variable\n",
        "                wayback_url = f\"{base_url}/web/{wayback_date}mp_/{canonical_url}\"\n",
        "                \n",
        "                # Verificar que no sea de los tipos excluidos\n",
        "                title = element.get('headlines', {}).get('basic', '').lower()\n",
        "                if not any(excluded.lower() in title for excluded in excluded_types):\n",
        "                    column_links.append(wayback_url)\n",
        "\n",
        "# Eliminar duplicados\n",
        "column_links = list(dict.fromkeys(column_links))\n",
        "\n",
        "# Mostrar resultados\n",
        "print(f\"\\nSe encontraron {len(column_links)} enlaces de columnas válidos:\")\n",
        "for i, link in enumerate(column_links, 1):\n",
        "    print(f\"{i}. {link}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
