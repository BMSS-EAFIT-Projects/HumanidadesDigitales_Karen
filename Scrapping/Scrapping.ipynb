{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "fraZdbMlRsMY"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from openpyxl import Workbook\n",
        "from openpyxl.utils.dataframe import dataframe_to_rows\n",
        "import time\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "uDWp2Vi9dZoA"
      },
      "outputs": [],
      "source": [
        "headers = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vZnwtGJVSPzS",
        "outputId": "14710ee5-9c2a-45c9-fb17-dfeaf9467bcd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Título: Independencia en ceros\n",
            "Fecha: 2 de marzo de 2018\n",
            "Autor: José Roberto Acosta\n",
            "\n",
            "--- CONTENIDO ---\n",
            "\n",
            "Hace una semana para nadie era un problema los tres ceros en nuestros billetes, pero para ocultar su inoperancia y conflicto de intereses en el caso Odebrecht, el fiscal general Néstor Humberto Martinez salió con tan costosa cortina de humo.\n",
            "\n",
            "Es sospechoso que, siendo legal y necesaria la independencia del fiscal general de la Nación respecto al Poder Ejecutivo del Estado, de manera simultánea saliera el ministro de Hacienda a decir que ya tenía listo el proyecto de ley parta tramitarlo en el Congreso generándole un gasto de por lo menos $400.000 millones a la nación, sin contar los costos en cambios y software y sistemas de contabilidad para las empresas y las millonadas que derrochará el Gobierno en campañas publicitarias de pedagogía por los tres años que duraría la transición al “nuevo peso”, tiempo suficiente para la operación de lavado de las caletas mencionadas por el fiscal.\n",
            "\n",
            "Es sospechoso que, debiendo autonomía e independencia con el Gobierno, demarcada por la propia Constitución Política de nuestra nación, el gerente general del Banco de la República saliera a coadyuvar tan inoportuna iniciativa, omitiendo que la elimninación de los tres ceros del billete generará un choque de inflación por cuenta del redondeo de precios al alza, contradiciendo su mandato constitucional de controlar el poder adquisitivo de los colombianos.\n",
            "\n",
            "Pero tan sospechoso concierto entre funcionarios de diferentes partidos políticos para impulsar una medida con indudables traumatismos sobre la economía y gran costo para unas finanzas públicas en déficit se empieza a explicar por la necesidad de desviar la atención de problemas más graves como el creciente desempleo, la bomba pensional, la inseguridad urbana y una creciente crisis del sistema público de salud que es saqueado permanentemente, como se advirtió con el caso de Medimás EPS, que ya lleva un mes sin pagarle a su propia red de hospitales, a pesar de recibir cerca de $300.000 millones mensuales y sin que la Superintendencia de Salud o el “tibio” ministro de Salud digan o hagan algo.\n",
            "\n",
            "Esta inaceptable manguala de funcionarios públicos, que deberían guardar distancia e independencia entre sí por mandato legal, solo prueba que al momento de hacer negocios con los dineros públicos los partidos políticos que han gobernado a Colombia durante los últimos 50 años simulan pelear en público pero cuadran su tajada en el poder.\n",
            "\n",
            "@jrobertoacosta1 , [email protected]\n"
          ]
        }
      ],
      "source": [
        "# URL del artículo en Wayback Machine\n",
        "url = \"https://web.archive.org/web/20180304015632/https://www.elespectador.com/opinion/independencia-en-ceros-columna-742192\"\n",
        "\n",
        "response = requests.get(url)\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "# Diccionario para meses en español\n",
        "meses = {\n",
        "    \"Ene\": \"enero\", \"Feb\": \"febrero\", \"Mar\": \"marzo\", \"Abr\": \"abril\",\n",
        "    \"May\": \"mayo\", \"Jun\": \"junio\", \"Jul\": \"julio\", \"Ago\": \"agosto\",\n",
        "    \"Sep\": \"septiembre\", \"Oct\": \"octubre\", \"Nov\": \"noviembre\", \"Dic\": \"diciembre\"\n",
        "}\n",
        "\n",
        "# 1. Extraer título\n",
        "titulo = soup.find('h1').get_text(strip=True)\n",
        "\n",
        "# 2. Extraer fecha formateada\n",
        "fecha_element = soup.find('div', class_='node-post-date')\n",
        "if fecha_element:\n",
        "    fecha_texto = fecha_element.get_text(strip=True).split(' - ')[0]\n",
        "    dia, mes_abrev, anio = fecha_texto.split()\n",
        "    fecha_formateada = f\"{dia} de {meses[mes_abrev]} de {anio}\"\n",
        "else:\n",
        "    fecha_formateada = \"Fecha no encontrada\"\n",
        "\n",
        "# 3. Extraer autor (texto después de \"Por:\")\n",
        "autor_element = soup.find('span', class_='by')  # Localiza el span con \"Por:\"\n",
        "if autor_element:\n",
        "    autor = autor_element.next_sibling.strip()  # Toma el texto HERMANO siguiente al span\n",
        "else:\n",
        "    autor = \"Autor no encontrado\"\n",
        "\n",
        "# 4. Extraer contenido limpio con párrafos (versión mejorada)\n",
        "contenido_div = soup.find('div', class_='node-body')\n",
        "if contenido_div:\n",
        "    # Primero eliminar el div no deseado si existe\n",
        "    info_node = contenido_div.find('div', class_='info_node_hide')\n",
        "    if info_node:\n",
        "        info_node.decompose()  # Esto elimina completamente el div y su contenido\n",
        "\n",
        "    # Eliminar solo elementos no deseados (scripts, iframes, etc.)\n",
        "    for element in contenido_div(['script', 'style', 'iframe', 'img', 'figure']):\n",
        "        element.decompose()\n",
        "\n",
        "    # Procesar cada párrafo conservando formato semántico\n",
        "    parrafos = []\n",
        "    for p in contenido_div.find_all('p'):\n",
        "        # Extraer todo el texto del párrafo incluyendo etiquetas de formato\n",
        "        texto_parrafo = p.get_text(' ', strip=True)  # El espacio une elementos separados\n",
        "        if texto_parrafo:\n",
        "            # Limpieza final de espacios múltiples\n",
        "            texto_parrafo = ' '.join(texto_parrafo.split())\n",
        "            parrafos.append(texto_parrafo)\n",
        "\n",
        "    contenido = '\\n\\n'.join(parrafos)\n",
        "else:\n",
        "    contenido = \"Contenido no encontrado\"\n",
        "\n",
        "# Resultados\n",
        "print(f\"Título: {titulo}\")\n",
        "print(f\"Fecha: {fecha_formateada}\")\n",
        "print(f\"Autor: {autor}\")\n",
        "print(\"\\n--- CONTENIDO ---\\n\")\n",
        "print(contenido)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yAi4JbyRb8XB",
        "outputId": "a01dffe9-981d-445d-e1a3-069f7bc1139c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Procesando: https://web.archive.org/web/20180728022353/https://www.elespectador.com/opinion/sismos-y-osos-columna-802506\n",
            "Procesando: https://web.archive.org/web/20180728022353/https://www.elespectador.com/opinion/una-prueba-acida-columna-802570\n",
            "Procesando: https://web.archive.org/web/20180728022353/https://www.elespectador.com/opinion/una-vida-sin-dios-columna-802571\n",
            "Procesando: https://web.archive.org/web/20180728022353/https://www.elespectador.com/opinion/si-la-vida-te-da-limones-columna-802572\n",
            "Procesando: https://web.archive.org/web/20180728022353/https://www.elespectador.com/opinion/luna-sangre-marte-agua-columna-802569\n",
            "Procesando: https://web.archive.org/web/20180728022353/https://www.elespectador.com/opinion/los-subsidios-estatales-columna-802573\n",
            "Procesando: https://web.archive.org/web/20180728022353/https://www.elespectador.com/opinion/la-consulta-columna-802516\n",
            "Procesando: https://web.archive.org/web/20180728022353/https://www.elespectador.com/opinion/comision-de-la-verdad-para-salud-columna-802507\n",
            "Procesando: https://web.archive.org/web/20180728022353/https://www.elespectador.com/opinion/cero-tolerancia-cualquier-tipo-de-violencia-contra-la-ninez-columna-802515\n",
            "Procesando: https://web.archive.org/web/20180728022353/https://www.elespectador.com/opinion/con-los-datos-del-sistema-publico-de-salud-no-se-juega-columna-802568\n",
            "Procesando: https://web.archive.org/web/20180728022353/https://www.elespectador.com/opinion/de-correa-uribe-columna-802505\n",
            "Procesando: https://web.archive.org/web/20180728022353/https://www.elespectador.com/opinion/las-cifras-cantan-columna-802487\n",
            "Procesando: https://web.archive.org/web/20180728022353/https://www.elespectador.com/opinion/no-atizar-el-fuego-columna-802508\n",
            "Procesando: https://web.archive.org/web/20180728022353/https://www.elespectador.com/opinion/el-arte-en-su-laberinto-columna-802811\n",
            "Procesando: https://web.archive.org/web/20180728022353/https://www.elespectador.com/opinion/sobre-ovnis-y-pruebas-irrefutables-columna-802514\n",
            "Procesando: https://web.archive.org/web/20180729030751/https://www.elespectador.com/opinion/obras-son-amores-columna-802724\n",
            "Procesando: https://web.archive.org/web/20180729030751/https://www.elespectador.com/opinion/hasta-cuando-las-cumbres-borrascosas-columna-802729\n",
            "Procesando: https://web.archive.org/web/20180729030751/https://www.elespectador.com/opinion/gracias-presidente-columna-802723\n",
            "Procesando: https://web.archive.org/web/20180729030751/https://www.elespectador.com/opinion/dos-siglos-de-edad-columna-802722\n",
            "Procesando: https://web.archive.org/web/20180729030751/https://www.elespectador.com/opinion/el-proximo-contralor-columna-802906\n",
            "Procesados: 20 de 60\n",
            "Procesando: https://web.archive.org/web/20180729030751/https://www.elespectador.com/opinion/el-pulso-final-columna-802908\n",
            "Procesando: https://web.archive.org/web/20180729030751/https://www.elespectador.com/opinion/desigualdad-para-mi-prima-ii-columna-802909\n",
            "Procesando: https://web.archive.org/web/20180729030751/https://www.elespectador.com/opinion/luto-marimbero-columna-802725\n",
            "Procesando: https://web.archive.org/web/20180729030751/https://www.elespectador.com/opinion/construir-no-es-oponerse-columna-802730\n",
            "Procesando: https://web.archive.org/web/20180729030751/https://www.elespectador.com/opinion/de-medimas-coomeva-columna-802706\n",
            "Procesando: https://web.archive.org/web/20180730035157/https://www.elespectador.com/opinion/liborio-la-voz-de-las-montanas-columna-802903\n",
            "Procesando: https://web.archive.org/web/20180730035157/https://www.elespectador.com/opinion/uno-dos-y-tres-columna-802902\n",
            "Procesando: https://web.archive.org/web/20180730035157/https://www.elespectador.com/opinion/vientos-de-guerra-columna-802889\n",
            "Procesando: https://web.archive.org/web/20180730035157/https://www.elespectador.com/opinion/evaluacion-de-maestros-columna-802888\n",
            "Procesando: https://web.archive.org/web/20180730035157/https://www.elespectador.com/opinion/desplazados-otra-vez-columna-802886\n",
            "Procesando: https://web.archive.org/web/20180730035157/https://www.elespectador.com/opinion/preguntas-al-contralor-general-columna-802904\n",
            "Error procesando https://web.archive.org/web/20180730035157/https://www.elespectador.com/opinion/preguntas-al-contralor-general-columna-802904: HTTPSConnectionPool(host='web.archive.org', port=443): Max retries exceeded with url: /web/20180730035157/https://www.elespectador.com/opinion/preguntas-al-contralor-general-columna-802904 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x0000022A2FB7AFC0>, 'Connection to web.archive.org timed out. (connect timeout=10)'))\n",
            "Procesando: https://web.archive.org/web/20180730035157/https://www.elespectador.com/opinion/la-muerte-del-corrector-columna-802900\n",
            "Procesando: https://web.archive.org/web/20180730035157/https://www.elespectador.com/opinion/carta-al-presidente-santos-columna-802905\n",
            "Procesando: https://web.archive.org/web/20180730035157/https://www.elespectador.com/opinion/un-somoza-del-siglo-xxi-columna-802898\n",
            "Procesando: https://web.archive.org/web/20180730035157/https://www.elespectador.com/opinion/la-inmortalidad-faraonica-columna-802979\n",
            "Procesando: https://web.archive.org/web/20180730035157/https://www.elespectador.com/opinion/dan-asco-columna-802860\n",
            "Procesando: https://web.archive.org/web/20180730035157/https://www.elespectador.com/opinion/10700-palabras-columna-802899\n",
            "Procesando: https://web.archive.org/web/20180730035157/https://www.elespectador.com/opinion/mientras-haya-coca-no-habra-paz-columna-802887\n",
            "Procesando: https://web.archive.org/web/20180730035157/https://www.elespectador.com/opinion/protesta-columna-802885\n",
            "Procesando: https://web.archive.org/web/20180730035157/https://www.elespectador.com/opinion/delicias-amargas-columna-802836\n",
            "Procesados: 40 de 60\n",
            "Procesando: https://web.archive.org/web/20180730035157/https://www.elespectador.com/opinion/propuesta-de-reactivacion-gremial-columna-802870\n",
            "Procesando: https://web.archive.org/web/20180731041501/https://www.elespectador.com/opinion/no-mas-tlc-columna-803130\n",
            "Procesando: https://web.archive.org/web/20180731041501/https://www.elespectador.com/opinion/santos-columna-803072\n",
            "Procesando: https://web.archive.org/web/20180731041501/https://www.elespectador.com/opinion/la-triste-historia-de-los-ductos-columna-803181\n",
            "Procesando: https://web.archive.org/web/20180731041501/https://www.elespectador.com/opinion/mayusculas-columna-803071\n",
            "Procesando: https://web.archive.org/web/20180731041501/https://www.elespectador.com/opinion/las-protestas-sociales-son-por-naturaleza-insubordinacion-las-reglas-columna-803128\n",
            "Procesando: https://web.archive.org/web/20180731041501/https://www.elespectador.com/opinion/una-justicia-excesivamente-cautelosa-columna-803127\n",
            "Procesando: https://web.archive.org/web/20180731041501/https://www.elespectador.com/opinion/congreso-para-farc-y-carcel-para-uribe-columna-803075\n",
            "Procesando: https://web.archive.org/web/20180731041501/https://www.elespectador.com/opinion/compas-de-espera-columna-803070\n",
            "Error procesando https://web.archive.org/web/20180731041501/https://www.elespectador.com/opinion/compas-de-espera-columna-803070: HTTPSConnectionPool(host='web.archive.org', port=443): Max retries exceeded with url: /web/20180731041501/https://www.elespectador.com/opinion/compas-de-espera-columna-803070 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x0000022A3048E3F0>, 'Connection to web.archive.org timed out. (connect timeout=10)'))\n",
            "Procesando: https://web.archive.org/web/20180731041501/https://www.elespectador.com/opinion/centroamericanos-manitos-columna-803088\n",
            "Procesando: https://web.archive.org/web/20180731041501/https://www.elespectador.com/opinion/meacabodeenterar-uribe-vs-corte-ambientalistas-silenciados-nairo-en-top-10-y-mas-columna-803308\n",
            "Procesando: https://web.archive.org/web/20180731041501/https://www.elespectador.com/opinion/la-gran-caceria-contra-alvaro-uribe-velez-columna-803129\n",
            "Procesando: https://web.archive.org/web/20180731041501/https://www.elespectador.com/opinion/tenemos-que-hablar-de-marihuana-columna-803132\n",
            "Procesando: https://web.archive.org/web/20180731041501/https://www.elespectador.com/opinion/despedida-con-libros-y-estampilla-columna-803074\n",
            "Procesando: https://web.archive.org/web/20180731041501/https://www.elespectador.com/opinion/colombia-40-la-cumbre-de-contenidos-digitales-para-el-mundo-columna-802874\n",
            "Procesando: https://web.archive.org/web/20180731041501/https://www.elespectador.com/opinion/combatir-la-ignorancia-columna-803131\n",
            "Procesando: https://web.archive.org/web/20180731041501/https://www.elespectador.com/opinion/7-de-agosto-de-2019-columna-803073\n",
            "Procesando: https://web.archive.org/web/20180731041501/https://www.elespectador.com/opinion/el-pacto-por-colombia-columna-803076\n",
            "Procesando: https://web.archive.org/web/20180801050335/https://www.elespectador.com/opinion/la-ultima-frontera-columna-803389\n",
            "Procesando: https://web.archive.org/web/20180801050335/https://www.elespectador.com/opinion/uribe-la-carcel-ya-casi-columna-803544\n",
            "\n",
            "Proceso completado. Resultados guardados en resultados_articulos.xlsx\n"
          ]
        }
      ],
      "source": [
        "# Usamos with para que el archivo se cierre automáticamente\n",
        "with open(\"urls.txt\", \"r\") as f:\n",
        "    # Leemos todas las líneas y filtramos las vacías\n",
        "    urls = [line.strip() for line in f if line.strip()]\n",
        "\n",
        "# Eliminamos duplicados\n",
        "urls = list(dict.fromkeys(urls))\n",
        "\n",
        "# Diccionario para meses en español\n",
        "meses = {\n",
        "    \"Ene\": \"enero\", \"Feb\": \"febrero\", \"Mar\": \"marzo\", \"Abr\": \"abril\",\n",
        "    \"May\": \"mayo\", \"Jun\": \"junio\", \"Jul\": \"julio\", \"Ago\": \"agosto\",\n",
        "    \"Sep\": \"septiembre\", \"Oct\": \"octubre\", \"Nov\": \"noviembre\", \"Dic\": \"diciembre\"\n",
        "}\n",
        "\n",
        "# Lista para almacenar todos los resultados\n",
        "datos = []\n",
        "n = 0\n",
        "for url in urls:\n",
        "    try:\n",
        "        print(f\"Procesando: {url}\")\n",
        "        response = requests.get(url, timeout=10)\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # 1. Extraer título\n",
        "        titulo = soup.find('h1').get_text(strip=True) if soup.find('h1') else \"Título no encontrado\"\n",
        "\n",
        "        # 2. Extraer fecha formateada\n",
        "        fecha_element = soup.find('div', class_='node-post-date')\n",
        "        if fecha_element:\n",
        "            fecha_texto = fecha_element.get_text(strip=True).split(' - ')[0]\n",
        "            try:\n",
        "                dia, mes_abrev, anio = fecha_texto.split()\n",
        "                fecha_formateada = f\"{dia} de {meses[mes_abrev]} de {anio}\"\n",
        "            except:\n",
        "                fecha_formateada = fecha_texto\n",
        "        else:\n",
        "            fecha_formateada = \"Fecha no encontrada\"\n",
        "\n",
        "        # 3. Extraer autor\n",
        "        autor_element = soup.find('span', class_='by')\n",
        "        if autor_element:\n",
        "            autor = autor_element.next_sibling.strip() if autor_element.next_sibling else \"Autor no encontrado\"\n",
        "        else:\n",
        "            autor = \"Autor no encontrado\"\n",
        "\n",
        "        # 4. Extraer contenido limpio con párrafos (versión mejorada)\n",
        "        contenido_div = soup.find('div', class_='node-body')\n",
        "        if contenido_div:\n",
        "            # Primero eliminar el div no deseado si existe\n",
        "            info_node = contenido_div.find('div', class_='info_node_hide')\n",
        "            if info_node:\n",
        "                info_node.decompose()  # Esto elimina completamente el div y su contenido\n",
        "\n",
        "            # Eliminar solo elementos no deseados (scripts, iframes, etc.)\n",
        "            for element in contenido_div(['script', 'style', 'iframe', 'img', 'figure']):\n",
        "                element.decompose()\n",
        "\n",
        "            # Procesar cada párrafo conservando formato semántico\n",
        "            parrafos = []\n",
        "            for p in contenido_div.find_all('p'):\n",
        "                # Extraer todo el texto del párrafo incluyendo etiquetas de formato\n",
        "                texto_parrafo = p.get_text(' ', strip=True)  # El espacio une elementos separados\n",
        "                if texto_parrafo:\n",
        "                    # Limpieza final de espacios múltiples\n",
        "                    texto_parrafo = ' '.join(texto_parrafo.split())\n",
        "                    parrafos.append(texto_parrafo)\n",
        "\n",
        "            contenido = '\\n\\n'.join(parrafos)\n",
        "        else:\n",
        "            contenido = \"Contenido no encontrado\"\n",
        "\n",
        "        # Agregar a la lista de datos\n",
        "        datos.append({\n",
        "            'Autor': autor,\n",
        "            'Fecha': fecha_formateada,\n",
        "            'Título': titulo,\n",
        "            'Contenido': contenido,\n",
        "            'URL': url\n",
        "        })\n",
        "        #time.sleep(1)\n",
        "    except Exception as e:\n",
        "        print(f\"Error procesando {url}: {str(e)}\")\n",
        "        datos.append({\n",
        "            'Autor': f\"Error: {str(e)}\",\n",
        "            'Fecha': \"\",\n",
        "            'Título': \"\",\n",
        "            'Contenido': \"\",\n",
        "            'URL': url\n",
        "        })\n",
        "    n = n + 1\n",
        "    if n % 20 == 0 and n < len(urls):\n",
        "        print(f\"Procesados: {n} de {len(urls)}\")\n",
        "        time.sleep(150)\n",
        "\n",
        "\n",
        "\n",
        "# Crear DataFrame y guardar como CSV\n",
        "df = pd.DataFrame(datos)\n",
        "\n",
        "# Ordenar columnas\n",
        "column_order = ['Autor', 'Fecha', 'Título', 'Contenido', 'URL']\n",
        "df = df[column_order]\n",
        "\n",
        "# Crear archivo Excel\n",
        "nombre_archivo = \"resultados_articulos.xlsx\"\n",
        "with pd.ExcelWriter(nombre_archivo, engine='openpyxl') as writer:\n",
        "    df.to_excel(writer, index=False, sheet_name='Artículos')\n",
        "\n",
        "    # Ajustar el ancho de las columnas\n",
        "    worksheet = writer.sheets['Artículos']\n",
        "    worksheet.column_dimensions['A'].width = 25  # Autor\n",
        "    worksheet.column_dimensions['B'].width = 20  # Fecha\n",
        "    worksheet.column_dimensions['C'].width = 40  # Título\n",
        "    worksheet.column_dimensions['D'].width = 80  # Contenido\n",
        "    worksheet.column_dimensions['E'].width = 60  # URL\n",
        "\n",
        "print(f\"\\nProceso completado. Resultados guardados en {nombre_archivo}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: requests-html in c:\\users\\karen\\anaconda3\\lib\\site-packages (0.10.0)\n",
            "Requirement already satisfied: requests in c:\\users\\karen\\anaconda3\\lib\\site-packages (from requests-html) (2.32.3)\n",
            "Requirement already satisfied: pyquery in c:\\users\\karen\\anaconda3\\lib\\site-packages (from requests-html) (2.0.1)\n",
            "Requirement already satisfied: fake-useragent in c:\\users\\karen\\anaconda3\\lib\\site-packages (from requests-html) (2.2.0)\n",
            "Requirement already satisfied: parse in c:\\users\\karen\\anaconda3\\lib\\site-packages (from requests-html) (1.20.2)\n",
            "Requirement already satisfied: bs4 in c:\\users\\karen\\anaconda3\\lib\\site-packages (from requests-html) (0.0.2)\n",
            "Requirement already satisfied: w3lib in c:\\users\\karen\\anaconda3\\lib\\site-packages (from requests-html) (2.1.2)\n",
            "Requirement already satisfied: pyppeteer>=0.0.14 in c:\\users\\karen\\anaconda3\\lib\\site-packages (from requests-html) (2.0.0)\n",
            "Requirement already satisfied: appdirs<2.0.0,>=1.4.3 in c:\\users\\karen\\anaconda3\\lib\\site-packages (from pyppeteer>=0.0.14->requests-html) (1.4.4)\n",
            "Requirement already satisfied: certifi>=2023 in c:\\users\\karen\\anaconda3\\lib\\site-packages (from pyppeteer>=0.0.14->requests-html) (2025.1.31)\n",
            "Requirement already satisfied: importlib-metadata>=1.4 in c:\\users\\karen\\anaconda3\\lib\\site-packages (from pyppeteer>=0.0.14->requests-html) (7.0.1)\n",
            "Requirement already satisfied: pyee<12.0.0,>=11.0.0 in c:\\users\\karen\\anaconda3\\lib\\site-packages (from pyppeteer>=0.0.14->requests-html) (11.1.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.42.1 in c:\\users\\karen\\anaconda3\\lib\\site-packages (from pyppeteer>=0.0.14->requests-html) (4.66.5)\n",
            "Requirement already satisfied: urllib3<2.0.0,>=1.25.8 in c:\\users\\karen\\anaconda3\\lib\\site-packages (from pyppeteer>=0.0.14->requests-html) (1.26.20)\n",
            "Requirement already satisfied: websockets<11.0,>=10.0 in c:\\users\\karen\\anaconda3\\lib\\site-packages (from pyppeteer>=0.0.14->requests-html) (10.4)\n",
            "Requirement already satisfied: beautifulsoup4 in c:\\users\\karen\\anaconda3\\lib\\site-packages (from bs4->requests-html) (4.12.3)\n",
            "Requirement already satisfied: lxml>=2.1 in c:\\users\\karen\\anaconda3\\lib\\site-packages (from pyquery->requests-html) (5.2.1)\n",
            "Requirement already satisfied: cssselect>=1.2.0 in c:\\users\\karen\\anaconda3\\lib\\site-packages (from pyquery->requests-html) (1.2.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\karen\\anaconda3\\lib\\site-packages (from requests->requests-html) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\karen\\anaconda3\\lib\\site-packages (from requests->requests-html) (3.7)\n",
            "Requirement already satisfied: zipp>=0.5 in c:\\users\\karen\\anaconda3\\lib\\site-packages (from importlib-metadata>=1.4->pyppeteer>=0.0.14->requests-html) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions in c:\\users\\karen\\anaconda3\\lib\\site-packages (from pyee<12.0.0,>=11.0.0->pyppeteer>=0.0.14->requests-html) (4.11.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\karen\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.42.1->pyppeteer>=0.0.14->requests-html) (0.4.6)\n",
            "Requirement already satisfied: soupsieve>1.2 in c:\\users\\karen\\anaconda3\\lib\\site-packages (from beautifulsoup4->bs4->requests-html) (2.5)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install requests-html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting requests-html\n",
            "  Using cached requests_html-0.10.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting requests (from requests-html)\n",
            "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting pyquery (from requests-html)\n",
            "  Using cached pyquery-2.0.1-py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting fake-useragent (from requests-html)\n",
            "  Using cached fake_useragent-2.2.0-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting parse (from requests-html)\n",
            "  Using cached parse-1.20.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting bs4 (from requests-html)\n",
            "  Using cached bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\n",
            "Collecting w3lib (from requests-html)\n",
            "  Downloading w3lib-2.3.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting pyppeteer>=0.0.14 (from requests-html)\n",
            "  Using cached pyppeteer-2.0.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting appdirs<2.0.0,>=1.4.3 (from pyppeteer>=0.0.14->requests-html)\n",
            "  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting certifi>=2023 (from pyppeteer>=0.0.14->requests-html)\n",
            "  Downloading certifi-2025.4.26-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting importlib-metadata>=1.4 (from pyppeteer>=0.0.14->requests-html)\n",
            "  Downloading importlib_metadata-8.7.0-py3-none-any.whl.metadata (4.8 kB)\n",
            "Collecting pyee<12.0.0,>=11.0.0 (from pyppeteer>=0.0.14->requests-html)\n",
            "  Using cached pyee-11.1.1-py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting tqdm<5.0.0,>=4.42.1 (from pyppeteer>=0.0.14->requests-html)\n",
            "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
            "Collecting urllib3<2.0.0,>=1.25.8 (from pyppeteer>=0.0.14->requests-html)\n",
            "  Using cached urllib3-1.26.20-py2.py3-none-any.whl.metadata (50 kB)\n",
            "Collecting websockets<11.0,>=10.0 (from pyppeteer>=0.0.14->requests-html)\n",
            "  Using cached websockets-10.4-cp312-cp312-win_amd64.whl\n",
            "Collecting beautifulsoup4 (from bs4->requests-html)\n",
            "  Downloading beautifulsoup4-4.13.4-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting lxml>=2.1 (from pyquery->requests-html)\n",
            "  Downloading lxml-5.4.0-cp312-cp312-win_amd64.whl.metadata (3.6 kB)\n",
            "Collecting cssselect>=1.2.0 (from pyquery->requests-html)\n",
            "  Downloading cssselect-1.3.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting charset-normalizer<4,>=2 (from requests->requests-html)\n",
            "  Downloading charset_normalizer-3.4.2-cp312-cp312-win_amd64.whl.metadata (36 kB)\n",
            "Collecting idna<4,>=2.5 (from requests->requests-html)\n",
            "  Downloading idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting zipp>=3.20 (from importlib-metadata>=1.4->pyppeteer>=0.0.14->requests-html)\n",
            "  Downloading zipp-3.21.0-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting typing-extensions (from pyee<12.0.0,>=11.0.0->pyppeteer>=0.0.14->requests-html)\n",
            "  Downloading typing_extensions-4.13.2-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting colorama (from tqdm<5.0.0,>=4.42.1->pyppeteer>=0.0.14->requests-html)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Collecting soupsieve>1.2 (from beautifulsoup4->bs4->requests-html)\n",
            "  Downloading soupsieve-2.7-py3-none-any.whl.metadata (4.6 kB)\n",
            "Using cached requests_html-0.10.0-py3-none-any.whl (13 kB)\n",
            "Using cached pyppeteer-2.0.0-py3-none-any.whl (82 kB)\n",
            "Using cached bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
            "Using cached fake_useragent-2.2.0-py3-none-any.whl (161 kB)\n",
            "Using cached parse-1.20.2-py2.py3-none-any.whl (20 kB)\n",
            "Using cached pyquery-2.0.1-py3-none-any.whl (22 kB)\n",
            "Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "Downloading w3lib-2.3.1-py3-none-any.whl (21 kB)\n",
            "Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
            "Downloading certifi-2025.4.26-py3-none-any.whl (159 kB)\n",
            "Downloading charset_normalizer-3.4.2-cp312-cp312-win_amd64.whl (105 kB)\n",
            "Downloading cssselect-1.3.0-py3-none-any.whl (18 kB)\n",
            "Downloading idna-3.10-py3-none-any.whl (70 kB)\n",
            "Downloading importlib_metadata-8.7.0-py3-none-any.whl (27 kB)\n",
            "Downloading lxml-5.4.0-cp312-cp312-win_amd64.whl (3.8 MB)\n",
            "   ---------------------------------------- 0.0/3.8 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/3.8 MB ? eta -:--:--\n",
            "   -- ------------------------------------- 0.3/3.8 MB ? eta -:--:--\n",
            "   -------- ------------------------------- 0.8/3.8 MB 1.3 MB/s eta 0:00:03\n",
            "   ---------- ----------------------------- 1.0/3.8 MB 1.3 MB/s eta 0:00:03\n",
            "   ------------- -------------------------- 1.3/3.8 MB 1.3 MB/s eta 0:00:02\n",
            "   ---------------- ----------------------- 1.6/3.8 MB 1.3 MB/s eta 0:00:02\n",
            "   ------------------- -------------------- 1.8/3.8 MB 1.3 MB/s eta 0:00:02\n",
            "   --------------------- ------------------ 2.1/3.8 MB 1.4 MB/s eta 0:00:02\n",
            "   ------------------------ --------------- 2.4/3.8 MB 1.3 MB/s eta 0:00:02\n",
            "   --------------------------- ------------ 2.6/3.8 MB 1.3 MB/s eta 0:00:01\n",
            "   ------------------------------ --------- 2.9/3.8 MB 1.3 MB/s eta 0:00:01\n",
            "   -------------------------------- ------- 3.1/3.8 MB 1.3 MB/s eta 0:00:01\n",
            "   -------------------------------------- - 3.7/3.8 MB 1.4 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 3.8/3.8 MB 1.4 MB/s eta 0:00:00\n",
            "Using cached pyee-11.1.1-py3-none-any.whl (15 kB)\n",
            "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
            "Using cached urllib3-1.26.20-py2.py3-none-any.whl (144 kB)\n",
            "Downloading beautifulsoup4-4.13.4-py3-none-any.whl (187 kB)\n",
            "Downloading soupsieve-2.7-py3-none-any.whl (36 kB)\n",
            "Downloading typing_extensions-4.13.2-py3-none-any.whl (45 kB)\n",
            "Downloading zipp-3.21.0-py3-none-any.whl (9.6 kB)\n",
            "Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Installing collected packages: parse, appdirs, zipp, websockets, w3lib, urllib3, typing-extensions, soupsieve, lxml, idna, fake-useragent, cssselect, colorama, charset-normalizer, certifi, tqdm, requests, pyquery, pyee, importlib-metadata, beautifulsoup4, pyppeteer, bs4, requests-html\n",
            "  Attempting uninstall: parse\n",
            "    Found existing installation: parse 1.20.2\n",
            "    Uninstalling parse-1.20.2:\n",
            "      Successfully uninstalled parse-1.20.2\n",
            "  Attempting uninstall: appdirs\n",
            "    Found existing installation: appdirs 1.4.4\n",
            "    Uninstalling appdirs-1.4.4:\n",
            "      Successfully uninstalled appdirs-1.4.4\n",
            "  Attempting uninstall: zipp\n",
            "    Found existing installation: zipp 3.17.0\n",
            "    Uninstalling zipp-3.17.0:\n",
            "      Successfully uninstalled zipp-3.17.0\n",
            "  Attempting uninstall: websockets\n",
            "    Found existing installation: websockets 10.4\n",
            "    Uninstalling websockets-10.4:\n",
            "      Successfully uninstalled websockets-10.4\n",
            "  Attempting uninstall: w3lib\n",
            "    Found existing installation: w3lib 2.1.2\n",
            "    Uninstalling w3lib-2.1.2:\n",
            "      Successfully uninstalled w3lib-2.1.2\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.26.20\n",
            "    Uninstalling urllib3-1.26.20:\n",
            "      Successfully uninstalled urllib3-1.26.20\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.11.0\n",
            "    Uninstalling typing_extensions-4.11.0:\n",
            "      Successfully uninstalled typing_extensions-4.11.0\n",
            "  Attempting uninstall: soupsieve\n",
            "    Found existing installation: soupsieve 2.5\n",
            "    Uninstalling soupsieve-2.5:\n",
            "      Successfully uninstalled soupsieve-2.5\n",
            "  Attempting uninstall: lxml\n",
            "    Found existing installation: lxml 5.2.1\n",
            "    Uninstalling lxml-5.2.1:\n",
            "      Successfully uninstalled lxml-5.2.1\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.7\n",
            "    Uninstalling idna-3.7:\n",
            "      Successfully uninstalled idna-3.7\n",
            "  Attempting uninstall: fake-useragent\n",
            "    Found existing installation: fake-useragent 2.2.0\n",
            "    Uninstalling fake-useragent-2.2.0:\n",
            "      Successfully uninstalled fake-useragent-2.2.0\n",
            "  Attempting uninstall: cssselect\n",
            "    Found existing installation: cssselect 1.2.0\n",
            "    Uninstalling cssselect-1.2.0:\n",
            "      Successfully uninstalled cssselect-1.2.0\n",
            "  Attempting uninstall: colorama\n",
            "    Found existing installation: colorama 0.4.6\n",
            "    Uninstalling colorama-0.4.6:\n",
            "      Successfully uninstalled colorama-0.4.6\n",
            "  Attempting uninstall: charset-normalizer\n",
            "    Found existing installation: charset-normalizer 3.3.2\n",
            "    Uninstalling charset-normalizer-3.3.2:\n",
            "      Successfully uninstalled charset-normalizer-3.3.2\n",
            "  Attempting uninstall: certifi\n",
            "    Found existing installation: certifi 2025.1.31\n",
            "    Uninstalling certifi-2025.1.31:\n",
            "      Successfully uninstalled certifi-2025.1.31\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.66.5\n",
            "    Uninstalling tqdm-4.66.5:\n",
            "      Successfully uninstalled tqdm-4.66.5\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.32.3\n",
            "    Uninstalling requests-2.32.3:\n",
            "      Successfully uninstalled requests-2.32.3\n",
            "  Attempting uninstall: pyquery\n",
            "    Found existing installation: pyquery 2.0.1\n",
            "    Uninstalling pyquery-2.0.1:\n",
            "      Successfully uninstalled pyquery-2.0.1\n",
            "  Attempting uninstall: pyee\n",
            "    Found existing installation: pyee 11.1.1\n",
            "    Uninstalling pyee-11.1.1:\n",
            "      Successfully uninstalled pyee-11.1.1\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib-metadata 7.0.1\n",
            "    Uninstalling importlib-metadata-7.0.1:\n",
            "      Successfully uninstalled importlib-metadata-7.0.1\n",
            "  Attempting uninstall: beautifulsoup4\n",
            "    Found existing installation: beautifulsoup4 4.12.3\n",
            "    Uninstalling beautifulsoup4-4.12.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.12.3\n",
            "  Attempting uninstall: pyppeteer\n",
            "    Found existing installation: pyppeteer 2.0.0\n",
            "    Uninstalling pyppeteer-2.0.0:\n",
            "      Successfully uninstalled pyppeteer-2.0.0\n",
            "  Attempting uninstall: bs4\n",
            "    Found existing installation: bs4 0.0.2\n",
            "    Uninstalling bs4-0.0.2:\n",
            "      Successfully uninstalled bs4-0.0.2\n",
            "  Attempting uninstall: requests-html\n",
            "    Found existing installation: requests-html 0.10.0\n",
            "    Uninstalling requests-html-0.10.0:\n",
            "      Successfully uninstalled requests-html-0.10.0\n",
            "Successfully installed appdirs-1.4.4 beautifulsoup4-4.13.4 bs4-0.0.2 certifi-2025.4.26 charset-normalizer-3.4.2 colorama-0.4.6 cssselect-1.3.0 fake-useragent-2.2.0 idna-3.10 importlib-metadata-8.7.0 lxml-5.4.0 parse-1.20.2 pyee-11.1.1 pyppeteer-2.0.0 pyquery-2.0.1 requests-2.32.3 requests-html-0.10.0 soupsieve-2.7 tqdm-4.67.1 typing-extensions-4.13.2 urllib3-1.26.20 w3lib-2.3.1 websockets-10.4 zipp-3.21.0\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\karen\\anaconda3\\Lib\\site-packages\\~xml'.\n",
            "  You can safely remove it manually.\n",
            "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "aext-assistant-server 4.1.0 requires anaconda-cloud-auth>=0.7.1, which is not installed.\n",
            "conda-repo-cli 1.0.114 requires urllib3>=2.2.2, but you have urllib3 1.26.20 which is incompatible.\n"
          ]
        }
      ],
      "source": [
        "pip install --force-reinstall requests-html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Procesando 1/1: https://web.archive.org/web/20200609164031mp_/https://www.elespectador.com/opinion/interrumpir-el-olvido/\n",
            "Error en https://web.archive.org/web/20200609164031mp_/https://www.elespectador.com/opinion/interrumpir-el-olvido/: Cannot use HTMLSession within an existing event loop. Use AsyncHTMLSession instead.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Título</th>\n",
              "      <th>Autor</th>\n",
              "      <th>Contenido</th>\n",
              "      <th>URL</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Error: Cannot use HTMLSession within an existi...</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>https://web.archive.org/web/20200609164031mp_/...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              Título Autor Contenido  \\\n",
              "0  Error: Cannot use HTMLSession within an existi...                   \n",
              "\n",
              "                                                 URL  \n",
              "0  https://web.archive.org/web/20200609164031mp_/...  "
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from requests_html import HTMLSession\n",
        "import pandas as pd\n",
        "from time import sleep\n",
        "\n",
        "# Iniciar sesión\n",
        "session = HTMLSession()\n",
        "\n",
        "def scrape_page(url):\n",
        "    try:\n",
        "        # Hacer la petición y renderizar JavaScript\n",
        "        r = session.get(url)\n",
        "        r.html.render(sleep=2, timeout=20)  # sleep espera 2 segundos después de cargar\n",
        "        \n",
        "        # Extraer datos (ajusta estos selectores según la nueva página)\n",
        "        titulo = r.html.find('.Article-Title', first=True).text if r.html.find('.Article-Title') else \"No encontrado\"\n",
        "        \n",
        "        # Buscar autor - ajusta el selector según la nueva estructura\n",
        "        autor = r.html.find('.author-name', first=True).text if r.html.find('.author-name') else \"Anónimo\"\n",
        "        \n",
        "        # Extraer contenido - ajusta el selector\n",
        "        parrafos = [p.text for p in r.html.find('article p')] if r.html.find('article p') else []\n",
        "        contenido = '\\n\\n'.join(parrafos) if parrafos else \"Contenido no disponible\"\n",
        "        \n",
        "        return {\n",
        "            'Título': titulo,\n",
        "            'Autor': autor,\n",
        "            'Contenido': contenido,\n",
        "            'URL': url\n",
        "        }\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Error en {url}: {str(e)}\")\n",
        "        return {\n",
        "            'Título': f\"Error: {str(e)}\",\n",
        "            'Autor': \"\",\n",
        "            'Contenido': \"\",\n",
        "            'URL': url\n",
        "        }\n",
        "\n",
        "# Lista de URLs (ejemplo)\n",
        "urls = [\n",
        "    \"https://web.archive.org/web/20200609164031mp_/https://www.elespectador.com/opinion/interrumpir-el-olvido/\",\n",
        "]\n",
        "\n",
        "# Procesar todas las URLs\n",
        "datos = []\n",
        "for i, url in enumerate(urls, 1):\n",
        "    print(f\"Procesando {i}/{len(urls)}: {url}\")\n",
        "    datos.append(scrape_page(url))\n",
        "    sleep(1)  # Pausa entre solicitudes\n",
        "\n",
        "# Guardar en Excel\n",
        "df = pd.DataFrame(datos)\n",
        "df\n",
        "#df.to_excel('articulos_elespectador.xlsx', index=False)\n",
        "#print(\"Scraping completado. Datos guardados en articulos_elespectador.xlsx\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Nuevo scrapping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "“Durante 20 días en Barcelona no hubo uno solo en el que, antes de abandonar el hotel, no me comiera al menos una pepa (...) Con frecuencia la explosión era tan fuerte que me tumbaba en el suelo y me dormía justo allí, donde me alcanzara el efecto. En ese momento no había dolor, todo fluía y yo era tan feliz como un niño en una piscina”. Alonso Sánchez Baute nos acaba de entregar, con su Parábola del salmón, un verdadero tsunami de emociones que nos arrastran página a página, línea tras línea, tocando fibras hondas y escondidas del alma, sacudiendo tripas y obligándonos a viajar con él al interior de nosotros mismos.\n",
            "\n",
            "Termino de leerlo y salgo a la superficie impactada. Lo llamo a felicitarlo y a decirle chapeau, pero es más profundo lo que siento. Le pregunto si es totalmente autobiográfico y me contesta que “si un libro tiene algún episodio de ficción, es ficción”. Pero, así no sean propios, todos los sucesos que nos comparte el narrador son historias auténticas. Y esa es la fuerza desgarradora de este libro. Fuerte y sin contemplaciones. Su periplo por Barcelona, Río de Janeiro, São Paulo y Buenos Aires es el recorrido de su alma, sus búsquedas, sus experiencias.\n",
            "\n",
            "Como el salmón. Ese pez misterioso que lucha contra las fuertes corrientes del río con una determinación y un conocimiento interno de que no permitirá que ningún obstáculo le impida su viaje hacia el origen. Ese pez que sabe bailar con sus miedos y seguir adelante a pesar de haber vivido dolores y experiencias extremas “sin soberbia en los ojos, solo sabiduría y piel gruesa”. Ese pez, monarca del río, capaz de recorrer más de 4.000 kilómetros hasta encontrar su origen en un recorrido vital lleno de enigmas y fantasía.\n",
            "\n",
            "Libro visceral. Sánchez Baute lo reconoce en una de sus páginas. Después de leer a Hervé Guibert, “se afianzó en mí la idea de que la escritura es un asunto de vísceras y de que, con tal de que sienta lo que lee, al lector hay que despedazarle al tiempo la cabeza y el corazón”. En otro aparte dice: “Escribir no fue una decisión. Ni siquiera lo pensé. Lo hice por necesidad de encontrar mi propia voz en un entorno que negaba la mía. En un pueblo misógino, un marica no era más que un mudo. De modo que desde muy joven enfrenté la disyuntiva de escribir o escribir. No había otra opción, escribía historias cargadas de terror porque era lo que sentía en ese momento: terror ante la vida, terror a que cualquiera supiera que habitaba en mí un monstruo que luchaba cada vez con más fuerzas para hacer añicos los barrotes”.\n",
            "\n",
            "Barcelona y su lujuria, el encuentro con “ese muchachito adolorido a la vera del camino, con ese dolor de agua estancada”. Río de Janeiro y esa espantosa sensación de soledad en la playa, recibiendo el Año Nuevo: “En ese momento, cuando el reloj marcó en punto las 12 de la noche y todos a mi alrededor corrieron a abrazarse y besarse, yo era menos que nada”. São Paulo, esa selva de cemento donde conoció la ternura: “Eso pasó cuando estaba con Thiago: aprendí que una sola persona en la que se pueda confiar nos libera”. Buenos Aires, “con ese calor de brea ardiente”, donde poco a poco se familiarizó con su noche, “donde la melancolía se asoma como protagonista”.\n",
            "\n",
            "Parábola del salmón, un libro sin máscaras, un recorrer contra la corriente, llegar al origen, cicatrizar heridas, sentir ese dolor sanador y pisar el infierno del sufrimiento para poder remontar. No tengo más remedio que decirle, de nuevo, chapeau.\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        " \n",
        "url = \"https://www.elespectador.com/opinion/columnistas/aura-lucia-mera/parabola-del-salmon-column/\"\n",
        " \n",
        "headers = {\n",
        "    \"User-Agent\": \"Mozilla/5.0\"\n",
        "}\n",
        " \n",
        "response = requests.get(url, headers=headers)\n",
        " \n",
        "if response.status_code == 200:\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        " \n",
        "    cuerpo = soup.find('div', class_='Article-Content')\n",
        "   \n",
        "    if cuerpo:\n",
        "        for basura in cuerpo.find_all(['audio', 'figure', 'figcaption', 'strong', 'div']):\n",
        "            basura.decompose()\n",
        " \n",
        "        parrafos = cuerpo.find_all('p')\n",
        "        texto = \"\\n\\n\".join(p.get_text(separator=\" \", strip=True) for p in parrafos)\n",
        " \n",
        "        texto = re.sub(r'\\s+([,.;:])', r'\\1', texto)\n",
        "        texto = re.sub(r\"([a-záéíóúñ])([A-ZÁÉÍÓÚÑ])\", r\"\\1 \\2\", texto)\n",
        " \n",
        "        print(texto)\n",
        "    else:\n",
        "        print(\"No se encontró el contenido del artículo.\")\n",
        "else:\n",
        "    print(f\"Error al acceder: código {response.status_code}\")\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Para extraer links"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Se encontraron 0 enlaces de columnas válidos:\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "fecha = \"20200616194558\"\n",
        "url = \"https://web.archive.org/web/\" + fecha + \"/https://www.elespectador.com/opinion/\"\n",
        "inicio = \"/web/\" + fecha + \"mp_/https://www.elespectador.com/\"\n",
        "\n",
        "try:\n",
        "    response = requests.get(url)\n",
        "    response.raise_for_status()\n",
        "except requests.exceptions.RequestException as e:\n",
        "    print(f\"Error al hacer la petición HTTP: {e}\")\n",
        "    exit()\n",
        "\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "column_items = soup.find_all('li', class_='views-row')\n",
        "\n",
        "column_links = []\n",
        "base_url = \"https://web.archive.org\"\n",
        "\n",
        "excluded_authors = [\n",
        "    \"Columnista invitado EE\",\n",
        "    \"Cartas de los lectores\",\n",
        "    \"Las igualadas\",\n",
        "    \"La Pulla\",\n",
        "    \"Antieditorial\",\n",
        "    \"Columna del lector\",\n",
        "    \"La Puesverdad\"\n",
        "]\n",
        "\n",
        "for item in column_items:\n",
        "    # Obtener el autor de manera más robusta\n",
        "    author = \"\"\n",
        "    author_div = item.find('div', class_='views-field-field-columnist')\n",
        "    if author_div:\n",
        "        author_link = author_div.find('a')\n",
        "        if author_link:\n",
        "            author = author_link.get_text(strip=True)\n",
        "        else:\n",
        "            author = author_div.get_text(strip=True).replace(\"Por: \", \"\")\n",
        "    \n",
        "    # Verificar si el autor está en la lista de exclusiones\n",
        "    if any(excluded.lower() in author.lower() for excluded in excluded_authors):\n",
        "        continue\n",
        "    \n",
        "    # Obtener el enlace de manera más segura\n",
        "    title_div = item.find('div', class_='views-field-title')\n",
        "    if not title_div:\n",
        "        continue\n",
        "        \n",
        "    title_link = title_div.find('a')\n",
        "    if not title_link or 'href' not in title_link.attrs:\n",
        "        continue\n",
        "    \n",
        "    href = title_link['href']\n",
        "    \n",
        "    # Verificar que sea un enlace de columna válido\n",
        "\n",
        "    #if (href.startswith(inicio) and 'columna-' in href):\n",
        "    if (href.startswith(inicio) in href):\n",
        "        full_url = base_url + href if not href.startswith(base_url) else href\n",
        "        column_links.append(full_url)\n",
        "\n",
        "# Eliminar duplicados manteniendo el orden\n",
        "seen = set()\n",
        "column_links = [x for x in column_links if not (x in seen or seen.add(x))]\n",
        "\n",
        "print(f\"\\nSe encontraron {len(column_links)} enlaces de columnas válidos:\")\n",
        "for i, link in enumerate(column_links, 1):\n",
        "    print(f\"{link}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Se encontraron 0 enlaces de columnas válidos:\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import json\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "\n",
        "url = \"https://web.archive.org/web/20200618142303/https://www.elespectador.com/opinion/\"\n",
        "\n",
        "try:\n",
        "    response = requests.get(url)\n",
        "    response.raise_for_status()\n",
        "except requests.exceptions.RequestException as e:\n",
        "    print(f\"Error al hacer la petición HTTP: {e}\")\n",
        "    exit()\n",
        "\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "# Buscar el script que contiene los datos JSON\n",
        "script_tag = soup.find('script', type='application/javascript')\n",
        "if not script_tag:\n",
        "    print(\"No se encontró el script con los datos JSON\")\n",
        "    exit()\n",
        "\n",
        "# Extraer el JSON del script\n",
        "json_data = {}\n",
        "try:\n",
        "    # Buscar la variable Fusion.globalContent\n",
        "    match = re.search(r'Fusion\\.globalContent\\s*=\\s*({.*?});', script_tag.string, re.DOTALL)\n",
        "    if match:\n",
        "        json_str = match.group(1)\n",
        "        json_data = json.loads(json_str)\n",
        "except (AttributeError, json.JSONDecodeError) as e:\n",
        "    print(f\"Error al procesar los datos JSON: {e}\")\n",
        "    exit()\n",
        "\n",
        "# Lista para almacenar los enlaces válidos\n",
        "column_links = []\n",
        "base_url = \"https://web.archive.org\"\n",
        "\n",
        "# Excluir estos tipos de contenido\n",
        "excluded_types = [\n",
        "    \"Columnista invitado EE\",\n",
        "    \"Cartas de los lectores\",\n",
        "    \"Las igualadas\",\n",
        "    \"La Pulla\",\n",
        "    \"Antieditorial\",\n",
        "    \"Columna del lector\",\n",
        "    \"La Puesverdad\"\n",
        "]\n",
        "\n",
        "# Procesar los elementos de contenido\n",
        "if 'content_elements' in json_data:\n",
        "    for element in json_data['content_elements']:\n",
        "        if element.get('type') == 'story':\n",
        "            # Obtener URL canónica\n",
        "            canonical_url = element.get('canonical_url', '')\n",
        "            \n",
        "            # Verificar que sea una columna de opinión\n",
        "            if canonical_url and '/opinion/' in canonical_url:\n",
        "                # Construir URL de Wayback Machine\n",
        "                wayback_date = \"20200618142303\"  # Puedes extraer esto de la URL original si es variable\n",
        "                wayback_url = f\"{base_url}/web/{wayback_date}mp_/{canonical_url}\"\n",
        "                \n",
        "                # Verificar que no sea de los tipos excluidos\n",
        "                title = element.get('headlines', {}).get('basic', '').lower()\n",
        "                if not any(excluded.lower() in title for excluded in excluded_types):\n",
        "                    column_links.append(wayback_url)\n",
        "\n",
        "# Eliminar duplicados\n",
        "column_links = list(dict.fromkeys(column_links))\n",
        "\n",
        "# Mostrar resultados\n",
        "print(f\"\\nSe encontraron {len(column_links)} enlaces de columnas válidos:\")\n",
        "for i, link in enumerate(column_links, 1):\n",
        "    print(f\"{i}. {link}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
