{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "fraZdbMlRsMY"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from openpyxl import Workbook\n",
        "from openpyxl.utils.dataframe import dataframe_to_rows\n",
        "import time\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "uDWp2Vi9dZoA"
      },
      "outputs": [],
      "source": [
        "headers = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vZnwtGJVSPzS",
        "outputId": "14710ee5-9c2a-45c9-fb17-dfeaf9467bcd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Título: Independencia en ceros\n",
            "Fecha: 2 de marzo de 2018\n",
            "Autor: José Roberto Acosta\n",
            "\n",
            "--- CONTENIDO ---\n",
            "\n",
            "Hace una semana para nadie era un problema los tres ceros en nuestros billetes, pero para ocultar su inoperancia y conflicto de intereses en el caso Odebrecht, el fiscal general Néstor Humberto Martinez salió con tan costosa cortina de humo.\n",
            "\n",
            "Es sospechoso que, siendo legal y necesaria la independencia del fiscal general de la Nación respecto al Poder Ejecutivo del Estado, de manera simultánea saliera el ministro de Hacienda a decir que ya tenía listo el proyecto de ley parta tramitarlo en el Congreso generándole un gasto de por lo menos $400.000 millones a la nación, sin contar los costos en cambios y software y sistemas de contabilidad para las empresas y las millonadas que derrochará el Gobierno en campañas publicitarias de pedagogía por los tres años que duraría la transición al “nuevo peso”, tiempo suficiente para la operación de lavado de las caletas mencionadas por el fiscal.\n",
            "\n",
            "Es sospechoso que, debiendo autonomía e independencia con el Gobierno, demarcada por la propia Constitución Política de nuestra nación, el gerente general del Banco de la República saliera a coadyuvar tan inoportuna iniciativa, omitiendo que la elimninación de los tres ceros del billete generará un choque de inflación por cuenta del redondeo de precios al alza, contradiciendo su mandato constitucional de controlar el poder adquisitivo de los colombianos.\n",
            "\n",
            "Pero tan sospechoso concierto entre funcionarios de diferentes partidos políticos para impulsar una medida con indudables traumatismos sobre la economía y gran costo para unas finanzas públicas en déficit se empieza a explicar por la necesidad de desviar la atención de problemas más graves como el creciente desempleo, la bomba pensional, la inseguridad urbana y una creciente crisis del sistema público de salud que es saqueado permanentemente, como se advirtió con el caso de Medimás EPS, que ya lleva un mes sin pagarle a su propia red de hospitales, a pesar de recibir cerca de $300.000 millones mensuales y sin que la Superintendencia de Salud o el “tibio” ministro de Salud digan o hagan algo.\n",
            "\n",
            "Esta inaceptable manguala de funcionarios públicos, que deberían guardar distancia e independencia entre sí por mandato legal, solo prueba que al momento de hacer negocios con los dineros públicos los partidos políticos que han gobernado a Colombia durante los últimos 50 años simulan pelear en público pero cuadran su tajada en el poder.\n",
            "\n",
            "@jrobertoacosta1 , [email protected]\n"
          ]
        }
      ],
      "source": [
        "# URL del artículo en Wayback Machine\n",
        "url = \"https://web.archive.org/web/20180304015632/https://www.elespectador.com/opinion/independencia-en-ceros-columna-742192\"\n",
        "\n",
        "response = requests.get(url)\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "# Diccionario para meses en español\n",
        "meses = {\n",
        "    \"Ene\": \"enero\", \"Feb\": \"febrero\", \"Mar\": \"marzo\", \"Abr\": \"abril\",\n",
        "    \"May\": \"mayo\", \"Jun\": \"junio\", \"Jul\": \"julio\", \"Ago\": \"agosto\",\n",
        "    \"Sep\": \"septiembre\", \"Oct\": \"octubre\", \"Nov\": \"noviembre\", \"Dic\": \"diciembre\"\n",
        "}\n",
        "\n",
        "# 1. Extraer título\n",
        "titulo = soup.find('h1').get_text(strip=True)\n",
        "\n",
        "# 2. Extraer fecha formateada\n",
        "fecha_element = soup.find('div', class_='node-post-date')\n",
        "if fecha_element:\n",
        "    fecha_texto = fecha_element.get_text(strip=True).split(' - ')[0]\n",
        "    dia, mes_abrev, anio = fecha_texto.split()\n",
        "    fecha_formateada = f\"{dia} de {meses[mes_abrev]} de {anio}\"\n",
        "else:\n",
        "    fecha_formateada = \"Fecha no encontrada\"\n",
        "\n",
        "# 3. Extraer autor (texto después de \"Por:\")\n",
        "autor_element = soup.find('span', class_='by')  # Localiza el span con \"Por:\"\n",
        "if autor_element:\n",
        "    autor = autor_element.next_sibling.strip()  # Toma el texto HERMANO siguiente al span\n",
        "else:\n",
        "    autor = \"Autor no encontrado\"\n",
        "\n",
        "# 4. Extraer contenido limpio con párrafos (versión mejorada)\n",
        "contenido_div = soup.find('div', class_='node-body')\n",
        "if contenido_div:\n",
        "    # Primero eliminar el div no deseado si existe\n",
        "    info_node = contenido_div.find('div', class_='info_node_hide')\n",
        "    if info_node:\n",
        "        info_node.decompose()  # Esto elimina completamente el div y su contenido\n",
        "\n",
        "    # Eliminar solo elementos no deseados (scripts, iframes, etc.)\n",
        "    for element in contenido_div(['script', 'style', 'iframe', 'img', 'figure']):\n",
        "        element.decompose()\n",
        "\n",
        "    # Procesar cada párrafo conservando formato semántico\n",
        "    parrafos = []\n",
        "    for p in contenido_div.find_all('p'):\n",
        "        # Extraer todo el texto del párrafo incluyendo etiquetas de formato\n",
        "        texto_parrafo = p.get_text(' ', strip=True)  # El espacio une elementos separados\n",
        "        if texto_parrafo:\n",
        "            # Limpieza final de espacios múltiples\n",
        "            texto_parrafo = ' '.join(texto_parrafo.split())\n",
        "            parrafos.append(texto_parrafo)\n",
        "\n",
        "    contenido = '\\n\\n'.join(parrafos)\n",
        "else:\n",
        "    contenido = \"Contenido no encontrado\"\n",
        "\n",
        "# Resultados\n",
        "print(f\"Título: {titulo}\")\n",
        "print(f\"Fecha: {fecha_formateada}\")\n",
        "print(f\"Autor: {autor}\")\n",
        "print(\"\\n--- CONTENIDO ---\\n\")\n",
        "print(contenido)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yAi4JbyRb8XB",
        "outputId": "a01dffe9-981d-445d-e1a3-069f7bc1139c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Procesando: https://web.archive.org/web/20181002102914/https://www.elespectador.com/opinion/el-caso-carrasquilla-columna-815545\n",
            "Procesando: https://web.archive.org/web/20181002102914/https://www.elespectador.com/opinion/el-castigo-charlie-mitten-columna-815416\n",
            "Procesando: https://web.archive.org/web/20181002102914/https://www.elespectador.com/opinion/ana-maria-archila-y-el-juez-kavanaugh-columna-815532\n",
            "Procesando: https://web.archive.org/web/20181002102914/https://www.elespectador.com/opinion/ciencia-y-universidad-publica-en-harapos-columna-815544\n",
            "Procesando: https://web.archive.org/web/20181002102914/https://www.elespectador.com/opinion/la-constatacion-de-la-impotencia-columna-815540\n",
            "Procesando: https://web.archive.org/web/20181002102914/https://www.elespectador.com/opinion/aporofobos-le-tenemos-fobia-los-pobres-columna-815547\n",
            "Procesando: https://web.archive.org/web/20181004164855/https://www.elespectador.com/opinion/tratar-bien-los-amigos-y-mal-los-enemigos-columna-815944\n",
            "Procesando: https://web.archive.org/web/20181004164855/https://www.elespectador.com/opinion/la-moda-sin-genero-no-es-una-moda-columna-816061\n",
            "Procesando: https://web.archive.org/web/20181004164855/https://www.elespectador.com/opinion/nos-quieren-quitar-el-derecho-al-aborto-y-en-nuestras-narices-columna-816060\n",
            "Procesando: https://web.archive.org/web/20181004164855/https://www.elespectador.com/opinion/el-suicidio-columna-816057\n",
            "Procesando: https://web.archive.org/web/20181004164855/https://www.elespectador.com/opinion/no-es-dios-columna-816056\n",
            "Procesando: https://web.archive.org/web/20181004164855/https://www.elespectador.com/opinion/podra-duque-reducir-el-consumo-de-drogas-ilegales-columna-816053\n",
            "Procesando: https://web.archive.org/web/20181005143735/https://www.elespectador.com/opinion/una-inocencia-poco-importante-columna-816174\n",
            "Procesando: https://web.archive.org/web/20181006173830/https://www.elespectador.com/opinion/al-vapor-del-glifosato-columna-816360\n",
            "Procesando: https://web.archive.org/web/20181007170338/https://www.elespectador.com/opinion/un-autor-busca-su-personaje-columna-816473\n",
            "Procesando: https://web.archive.org/web/20181007170338/https://www.elespectador.com/opinion/una-sobredosis-de-insensatez-columna-816474\n",
            "Procesando: https://web.archive.org/web/20181007170338/https://www.elespectador.com/opinion/carlos-mattos-y-sus-andanzas-columna-816488\n",
            "Procesando: https://web.archive.org/web/20181009210359/https://www.elespectador.com/opinion/encuentro-afrofemenino-columna-816823\n",
            "Procesando: https://web.archive.org/web/20181011232906/https://www.elespectador.com/opinion/nadia-murad-y-la-rosa-blanca-columna-817350\n",
            "Procesando: https://web.archive.org/web/20181011232906/https://www.elespectador.com/opinion/retornando-la-alma-mater-columna-817248\n",
            "Procesados: 20 de 42\n",
            "Procesando: https://web.archive.org/web/20181012122234/https://www.elespectador.com/opinion/la-universidad-publica-columna-817551\n",
            "Procesando: https://web.archive.org/web/20181015004106/https://www.elespectador.com/opinion/provincianos-columna-817788\n",
            "Error procesando https://web.archive.org/web/20181015004106/https://www.elespectador.com/opinion/provincianos-columna-817788: HTTPSConnectionPool(host='web.archive.org', port=443): Max retries exceeded with url: /web/20181015004106/https://www.elespectador.com/opinion/provincianos-columna-817788 (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000002274C9CD7F0>: Failed to establish a new connection: [WinError 10061] No se puede establecer una conexión ya que el equipo de destino denegó expresamente dicha conexión'))\n",
            "Procesando: https://web.archive.org/web/20181015004106/https://www.elespectador.com/opinion/la-consulta-previa-columna-817789\n",
            "Procesando: https://web.archive.org/web/20181016075001/https://www.elespectador.com/opinion/la-vida-nunca-es-gratis-columna-818103\n",
            "Procesando: https://web.archive.org/web/20181016075001/https://www.elespectador.com/opinion/como-marca-goles-el-deportes-tolima-columna-818124\n",
            "Procesando: https://web.archive.org/web/20181016075001/https://www.elespectador.com/opinion/un-trino-de-paloma-columna-818168\n",
            "Procesando: https://web.archive.org/web/20181016075001/https://www.elespectador.com/opinion/el-negacionismo-como-directriz-del-centro-de-memoria-historica-columna-818065\n",
            "Procesando: https://web.archive.org/web/20181018185316/https://www.elespectador.com/opinion/viejasverdes-columna-818457\n",
            "Procesando: https://web.archive.org/web/20181018185316/https://www.elespectador.com/opinion/no-mataras-columna-818556\n",
            "Procesando: https://web.archive.org/web/20181018185316/https://www.elespectador.com/opinion/ultima-columna-columna-818466\n",
            "Procesando: https://web.archive.org/web/20181019210316/https://www.elespectador.com/opinion/duque-en-busca-del-poder-columna-818659\n",
            "Procesando: https://web.archive.org/web/20181019210316/https://www.elespectador.com/opinion/rita-y-camille-columna-818729\n",
            "Procesando: https://web.archive.org/web/20181021232605/https://www.elespectador.com/opinion/sin-titulo-columna-819033\n",
            "Procesando: https://web.archive.org/web/20181022052701/https://www.elespectador.com/opinion/dayro-y-manga-no-son-los-culpables-columna-819227\n",
            "Procesando: https://web.archive.org/web/20181026073427/https://www.elespectador.com/opinion/el-futuro-de-la-ciencia-abierta-columna-820149\n",
            "Procesando: https://web.archive.org/web/20181029014511/https://www.elespectador.com/opinion/calor-y-aprendizaje-columna-820457\n",
            "Procesando: https://web.archive.org/web/20181028033351/https://www.elespectador.com/opinion/economia-para-pablo-columna-820482\n",
            "Procesando: https://web.archive.org/web/20181028033351/https://www.elespectador.com/opinion/desenfocados-columna-820477\n",
            "Procesando: https://web.archive.org/web/20181028033351/https://www.elespectador.com/opinion/generacion-e-y-el-futuro-de-la-gratuidad-en-educacion-superior-columna-820484\n",
            "Procesando: https://web.archive.org/web/20181030232125/https://www.elespectador.com/opinion/seguridad-en-medellin-cuando-las-cifras-contradicen-las-percepciones-columna-820647\n",
            "Procesados: 40 de 42\n",
            "Procesando: https://web.archive.org/web/20181030232125/https://www.elespectador.com/opinion/sobre-la-cadena-perpetua-columna-820651\n",
            "Procesando: https://web.archive.org/web/20181031234908/https://www.elespectador.com/opinion/somos-mayoria-columna-821027\n",
            "\n",
            "Proceso completado. Resultados guardados en resultados_articulos.xlsx\n"
          ]
        }
      ],
      "source": [
        "# ESTE es el que estoy usando para procesar múltiples URLs normales\n",
        "\n",
        "# Usamos with para que el archivo se cierre automáticamente\n",
        "with open(\"urls.txt\", \"r\") as f:\n",
        "    # Leemos todas las líneas y filtramos las vacías\n",
        "    urls = [line.strip() for line in f if line.strip()]\n",
        "\n",
        "# Eliminamos duplicados\n",
        "urls = list(dict.fromkeys(urls))\n",
        "\n",
        "# Diccionario para meses en español\n",
        "meses = {\n",
        "    \"Ene\": \"enero\", \"Feb\": \"febrero\", \"Mar\": \"marzo\", \"Abr\": \"abril\",\n",
        "    \"May\": \"mayo\", \"Jun\": \"junio\", \"Jul\": \"julio\", \"Ago\": \"agosto\",\n",
        "    \"Sep\": \"septiembre\", \"Oct\": \"octubre\", \"Nov\": \"noviembre\", \"Dic\": \"diciembre\"\n",
        "}\n",
        "\n",
        "# Lista para almacenar todos los resultados\n",
        "datos = []\n",
        "n = 0\n",
        "for url in urls:\n",
        "    try:\n",
        "        print(f\"Procesando: {url}\")\n",
        "        response = requests.get(url, timeout=10)\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # 1. Extraer título\n",
        "        titulo = soup.find('h1').get_text(strip=True) if soup.find('h1') else \"Título no encontrado\"\n",
        "\n",
        "        # 2. Extraer fecha formateada\n",
        "        fecha_element = soup.find('div', class_='node-post-date')\n",
        "        if fecha_element:\n",
        "            fecha_texto = fecha_element.get_text(strip=True).split(' - ')[0]\n",
        "            try:\n",
        "                dia, mes_abrev, anio = fecha_texto.split()\n",
        "                fecha_formateada = f\"{dia} de {meses[mes_abrev]} de {anio}\"\n",
        "            except:\n",
        "                fecha_formateada = fecha_texto\n",
        "        else:\n",
        "            fecha_formateada = \"Fecha no encontrada\"\n",
        "\n",
        "        # 3. Extraer autor\n",
        "        autor_element = soup.find('span', class_='by')\n",
        "        if autor_element:\n",
        "            autor = autor_element.next_sibling.strip() if autor_element.next_sibling else \"Autor no encontrado\"\n",
        "        else:\n",
        "            autor = \"Autor no encontrado\"\n",
        "\n",
        "        # 4. Extraer contenido limpio con párrafos (versión mejorada)\n",
        "        contenido_div = soup.find('div', class_='node-body')\n",
        "        if contenido_div:\n",
        "            # Primero eliminar el div no deseado si existe\n",
        "            info_node = contenido_div.find('div', class_='info_node_hide')\n",
        "            if info_node:\n",
        "                info_node.decompose()  # Esto elimina completamente el div y su contenido\n",
        "\n",
        "            # Eliminar solo elementos no deseados (scripts, iframes, etc.)\n",
        "            for element in contenido_div(['script', 'style', 'iframe', 'img', 'figure']):\n",
        "                element.decompose()\n",
        "\n",
        "            # Procesar cada párrafo conservando formato semántico\n",
        "            parrafos = []\n",
        "            for p in contenido_div.find_all('p'):\n",
        "                # Extraer todo el texto del párrafo incluyendo etiquetas de formato\n",
        "                texto_parrafo = p.get_text(' ', strip=True)  # El espacio une elementos separados\n",
        "                if texto_parrafo:\n",
        "                    # Limpieza final de espacios múltiples\n",
        "                    texto_parrafo = ' '.join(texto_parrafo.split())\n",
        "                    parrafos.append(texto_parrafo)\n",
        "\n",
        "            contenido = '\\n\\n'.join(parrafos)\n",
        "        else:\n",
        "            contenido = \"Contenido no encontrado\"\n",
        "\n",
        "        # Agregar a la lista de datos\n",
        "        datos.append({\n",
        "            'Autor': autor,\n",
        "            'Fecha': fecha_formateada,\n",
        "            'Título': titulo,\n",
        "            'Contenido': contenido,\n",
        "            'URL': url\n",
        "        })\n",
        "        #time.sleep(1)\n",
        "    except Exception as e:\n",
        "        print(f\"Error procesando {url}: {str(e)}\")\n",
        "        datos.append({\n",
        "            'Autor': f\"Error: {str(e)}\",\n",
        "            'Fecha': \"\",\n",
        "            'Título': \"\",\n",
        "            'Contenido': \"\",\n",
        "            'URL': url\n",
        "        })\n",
        "    n = n + 1\n",
        "    if n % 20 == 0 and n < len(urls):\n",
        "        print(f\"Procesados: {n} de {len(urls)}\")\n",
        "        time.sleep(150)\n",
        "\n",
        "\n",
        "\n",
        "# Crear DataFrame y guardar como CSV\n",
        "df = pd.DataFrame(datos)\n",
        "\n",
        "# Ordenar columnas\n",
        "column_order = ['Autor', 'Fecha', 'Título', 'Contenido', 'URL']\n",
        "df = df[column_order]\n",
        "\n",
        "# Crear archivo Excel\n",
        "nombre_archivo = \"resultados_articulos.xlsx\"\n",
        "with pd.ExcelWriter(nombre_archivo, engine='openpyxl') as writer:\n",
        "    df.to_excel(writer, index=False, sheet_name='Artículos')\n",
        "\n",
        "    # Ajustar el ancho de las columnas\n",
        "    worksheet = writer.sheets['Artículos']\n",
        "    worksheet.column_dimensions['A'].width = 25  # Autor\n",
        "    worksheet.column_dimensions['B'].width = 20  # Fecha\n",
        "    worksheet.column_dimensions['C'].width = 40  # Título\n",
        "    worksheet.column_dimensions['D'].width = 80  # Contenido\n",
        "    worksheet.column_dimensions['E'].width = 60  # URL\n",
        "\n",
        "print(f\"\\nProceso completado. Resultados guardados en {nombre_archivo}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Para extraer links"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Se encontraron 0 enlaces de columnas válidos:\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "fecha = \"20200616194558\"\n",
        "url = \"https://web.archive.org/web/\" + fecha + \"/https://www.elespectador.com/opinion/\"\n",
        "inicio = \"/web/\" + fecha + \"mp_/https://www.elespectador.com/\"\n",
        "\n",
        "try:\n",
        "    response = requests.get(url)\n",
        "    response.raise_for_status()\n",
        "except requests.exceptions.RequestException as e:\n",
        "    print(f\"Error al hacer la petición HTTP: {e}\")\n",
        "    exit()\n",
        "\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "column_items = soup.find_all('li', class_='views-row')\n",
        "\n",
        "column_links = []\n",
        "base_url = \"https://web.archive.org\"\n",
        "\n",
        "excluded_authors = [\n",
        "    \"Columnista invitado EE\",\n",
        "    \"Cartas de los lectores\",\n",
        "    \"Las igualadas\",\n",
        "    \"La Pulla\",\n",
        "    \"Antieditorial\",\n",
        "    \"Columna del lector\",\n",
        "    \"La Puesverdad\"\n",
        "]\n",
        "\n",
        "for item in column_items:\n",
        "    # Obtener el autor de manera más robusta\n",
        "    author = \"\"\n",
        "    author_div = item.find('div', class_='views-field-field-columnist')\n",
        "    if author_div:\n",
        "        author_link = author_div.find('a')\n",
        "        if author_link:\n",
        "            author = author_link.get_text(strip=True)\n",
        "        else:\n",
        "            author = author_div.get_text(strip=True).replace(\"Por: \", \"\")\n",
        "    \n",
        "    # Verificar si el autor está en la lista de exclusiones\n",
        "    if any(excluded.lower() in author.lower() for excluded in excluded_authors):\n",
        "        continue\n",
        "    \n",
        "    # Obtener el enlace de manera más segura\n",
        "    title_div = item.find('div', class_='views-field-title')\n",
        "    if not title_div:\n",
        "        continue\n",
        "        \n",
        "    title_link = title_div.find('a')\n",
        "    if not title_link or 'href' not in title_link.attrs:\n",
        "        continue\n",
        "    \n",
        "    href = title_link['href']\n",
        "    \n",
        "    # Verificar que sea un enlace de columna válido\n",
        "\n",
        "    #if (href.startswith(inicio) and 'columna-' in href):\n",
        "    if (href.startswith(inicio) in href):\n",
        "        full_url = base_url + href if not href.startswith(base_url) else href\n",
        "        column_links.append(full_url)\n",
        "\n",
        "# Eliminar duplicados manteniendo el orden\n",
        "seen = set()\n",
        "column_links = [x for x in column_links if not (x in seen or seen.add(x))]\n",
        "\n",
        "print(f\"\\nSe encontraron {len(column_links)} enlaces de columnas válidos:\")\n",
        "for i, link in enumerate(column_links, 1):\n",
        "    print(f\"{link}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Se encontraron 0 enlaces de columnas válidos:\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import json\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "\n",
        "url = \"https://web.archive.org/web/20200618142303/https://www.elespectador.com/opinion/\"\n",
        "\n",
        "try:\n",
        "    response = requests.get(url)\n",
        "    response.raise_for_status()\n",
        "except requests.exceptions.RequestException as e:\n",
        "    print(f\"Error al hacer la petición HTTP: {e}\")\n",
        "    exit()\n",
        "\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "# Buscar el script que contiene los datos JSON\n",
        "script_tag = soup.find('script', type='application/javascript')\n",
        "if not script_tag:\n",
        "    print(\"No se encontró el script con los datos JSON\")\n",
        "    exit()\n",
        "\n",
        "# Extraer el JSON del script\n",
        "json_data = {}\n",
        "try:\n",
        "    # Buscar la variable Fusion.globalContent\n",
        "    match = re.search(r'Fusion\\.globalContent\\s*=\\s*({.*?});', script_tag.string, re.DOTALL)\n",
        "    if match:\n",
        "        json_str = match.group(1)\n",
        "        json_data = json.loads(json_str)\n",
        "except (AttributeError, json.JSONDecodeError) as e:\n",
        "    print(f\"Error al procesar los datos JSON: {e}\")\n",
        "    exit()\n",
        "\n",
        "# Lista para almacenar los enlaces válidos\n",
        "column_links = []\n",
        "base_url = \"https://web.archive.org\"\n",
        "\n",
        "# Excluir estos tipos de contenido\n",
        "excluded_types = [\n",
        "    \"Columnista invitado EE\",\n",
        "    \"Cartas de los lectores\",\n",
        "    \"Las igualadas\",\n",
        "    \"La Pulla\",\n",
        "    \"Antieditorial\",\n",
        "    \"Columna del lector\",\n",
        "    \"La Puesverdad\"\n",
        "]\n",
        "\n",
        "# Procesar los elementos de contenido\n",
        "if 'content_elements' in json_data:\n",
        "    for element in json_data['content_elements']:\n",
        "        if element.get('type') == 'story':\n",
        "            # Obtener URL canónica\n",
        "            canonical_url = element.get('canonical_url', '')\n",
        "            \n",
        "            # Verificar que sea una columna de opinión\n",
        "            if canonical_url and '/opinion/' in canonical_url:\n",
        "                # Construir URL de Wayback Machine\n",
        "                wayback_date = \"20200618142303\"  # Puedes extraer esto de la URL original si es variable\n",
        "                wayback_url = f\"{base_url}/web/{wayback_date}mp_/{canonical_url}\"\n",
        "                \n",
        "                # Verificar que no sea de los tipos excluidos\n",
        "                title = element.get('headlines', {}).get('basic', '').lower()\n",
        "                if not any(excluded.lower() in title for excluded in excluded_types):\n",
        "                    column_links.append(wayback_url)\n",
        "\n",
        "# Eliminar duplicados\n",
        "column_links = list(dict.fromkeys(column_links))\n",
        "\n",
        "# Mostrar resultados\n",
        "print(f\"\\nSe encontraron {len(column_links)} enlaces de columnas válidos:\")\n",
        "for i, link in enumerate(column_links, 1):\n",
        "    print(f\"{i}. {link}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
